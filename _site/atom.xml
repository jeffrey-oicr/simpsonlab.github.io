<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Simpson Lab Blog</title>
 <link href="http://simpsonlab.github.io/atom.xml" rel="self"/>
 <link href="http://simpsonlab.github.io/"/>
 <updated>2016-02-02T14:20:27-08:00</updated>
 <id>simpsonlab.github.io</id>
 <author>
   <name>Jared Simpson</name>
   <email></email>
 </author>

 
  
   <entry>
     <title>Approximate Mapping of Nanopore Squiggle Data with Spatial Indexing</title>
     <link href="http://simpsonlab.github.io/2015/12/18/kdtree-mapping/"/>
     <updated>2015-12-18T00:00:00-08:00</updated>
     <id>simpsonlab.github.io/2015/12/18/kdtree-mapping</id>
     <content type="html"><h3 id="nanopore-data">Nanopore data</h3>

<p>Here at the Simpsonlab we work with signal-level data from nanopore
sequencers, particularly those from ONT like the MinION.  Signal-level
data looks something like this:</p>

<p><img src="/assets/kdtreemapping/squiggle.png" alt="Idealized Example Squiggle Plot" /></p>

<p>where the data comes in as a stream of events (the red lines) with
means, standard deviations, and durations, with each black point
being an individual signal reading.  We use the signal-level data
partly for accuracy - because converting each red line and dozens
of black points into one of four letters invariably loses lots
of information - and partly because those basecalls might not be
available in, say, the field, where the portability of the nanopore
sequencers really shines.</p>

<p>This data has to be interpreted in terms of a particular model of
the interaction of the pore and the strand of DNA; such a pore model
gives, amongst other things, an expected signal mean and standard
deviation for currents through the pore when a given <script type="math/tex">k</script>mer is in
the pore (for the MinION devices, pore models typically use <script type="math/tex">k</script>=5 or
6).  So a pore model might be, for <script type="math/tex">k</script>=5, a table with 1024 entries
that looks something like this:</p>

<table>
  <thead>
    <tr>
      <th>kmer</th>
      <th>mean</th>
      <th>std dev</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AAAAA</td>
      <td>70.24</td>
      <td>± 0.95 pA</td>
    </tr>
    <tr>
      <td>AAAAC</td>
      <td>66.13</td>
      <td>± 0.75 pA</td>
    </tr>
    <tr>
      <td>AAAAG</td>
      <td>70.23</td>
      <td>± 0.76 pA</td>
    </tr>
    <tr>
      <td>AAAAT</td>
      <td>69.25</td>
      <td>± 0.68 pA</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
  </tbody>
</table>

<p>and using such a pore model it’s fairly straightforward to
go from a sequence to an expected set of signal levels from the
sequencer: so, say, for the 10-base sequence AAAAACGTCC and the
above 5-mer pore model we’d expect 6 events (10-5+1) that look like:</p>

<table>
  <thead>
    <tr>
      <th>event</th>
      <th>kmer</th>
      <th>mean</th>
      <th>range (1<script type="math/tex">\sigma</script>)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>e1</em></td>
      <td>AAAAA</td>
      <td>70.2 pA</td>
      <td>69.3 -  71.2 pA</td>
    </tr>
    <tr>
      <td><em>e2</em></td>
      <td>AAAAC</td>
      <td>66.1 pA</td>
      <td>65.4 -  66.9 pA</td>
    </tr>
    <tr>
      <td><em>e3</em></td>
      <td>AAACG</td>
      <td>62.8 pA</td>
      <td>62.0 -  63.6 pA</td>
    </tr>
    <tr>
      <td><em>e4</em></td>
      <td>AACGT</td>
      <td>67.6 pA</td>
      <td>66.3 -  68.9 pA</td>
    </tr>
    <tr>
      <td><em>e5</em></td>
      <td>ACGTC</td>
      <td>56.6 pA</td>
      <td>54.5 -  58.7 pA</td>
    </tr>
    <tr>
      <td><em>e6</em></td>
      <td>CGTCC</td>
      <td>49.9 pA</td>
      <td>49.1 -  50.7 pA</td>
    </tr>
  </tbody>
</table>

<p>Using those models, plus HMMs and a tonne of math, Jared’s
<a href="https://github.com/jts/nanopolish">nanopolish</a> tool has had enormous
success improving the quality of nanopore reads and assemblies of
nanopore data.  As one can see from the ‘range’ column above, and
from the histogram on the side of the first plot, which shows the
numbers of kmers which fall into 1pA bins in the pore model, this
isn’t necessarily particularly easy.  There are over 70 kmers which
are expected to have means in the range 69.5pA - 70.5pA;
that plus the wide standard deviations means that there is enormous
ambiguity going back from signal levels to sequence.</p>

<h3 id="mapping">Mapping</h3>

<p>This ambiguity introduces complication into another part of the
bioinformatics pipeline - mapping.  While several mappers (BWA MEM,
LAST) work quite well with basecalled ONT data, and one mapper<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>
has been written specifically for basecalled ONT data, mapping
signal-level data directly is a little tougher.  It is one thing
for a seed sequence in the read to occur in multiple locations in
the reference index; it is another thing for a sequence of
floating-point currents to plausibly represent one of hundreds of
seed sequences.</p>

<p>On the other hand, a flurry of recent important papers and software<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>
<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> <sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup> <sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>, the first of which are discussed in some informative
<a href="http://robpatro.com/blog/?p=248">blog</a>
<a href="https://liorpachter.wordpress.com/2015/11/01/what-is-a-read-mapping/">posts</a>,
have demonstrated the usefulness of approximate read mapping.  In
our case, mapping to an approximate region of a reference would
allow exact but computationally expensive methods like <code class="highlighter-rouge">nanopolish
eventalign</code> to produce precise alignments, or simple assignment may
be all that is necessary for other applications.</p>

<p>So an output approximate mapping would be valuable, but the issues
remains of how to disambiguate the signal level values.</p>

<h3 id="spatial-indexing">Spatial Indexing</h3>

<p>Importantly, while any individual event is very difficult to assign
with precision, sequences of events are less ambiguous, as any given
event is followed, with high probability, by one of only four other
events.  Thus, by examining tuples of events, one can greatly
increase specificity.</p>

<p>And there are well-developed sets of techniques for looking up lists
of <script type="math/tex">d</script> floating point values to look for candidate matches: <a href="https://en.wikipedia.org/wiki/Spatial_database">spatial
indexes</a>, where
each query or match is considered as a point in <script type="math/tex">d</script>-dimensional
space, and the goal is to return either some number of nearest
points (k-Nearest-Neighbours, or <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">kNN queries</a>), or
all matches within some, typically euclidean, distance.</p>

<p>To see how this would work, consider indexing the sequence above,
AAAAACGTCC, in a spatial index with <script type="math/tex">d=2</script>.  There are a total of 6
events, so we’d have 5 points (6-2+1), each points in 2-dimensional
space; 4 are plotted below (the other falls out of the range of the
plot)</p>

<p><img src="/assets/kdtreemapping/2d-spatial-query.png" alt="2D Spatial Query" style="width: 478px; margin-left:auto; margin-right:auto;" /></p>

<p>and a query for <script type="math/tex">d</script> read events that could be matched by (69.5, 67), the blue point, would return the nearest match (70.2, 66.1) corresponding to the <script type="math/tex">k+d-1</script>-mer AAAAAC.</p>

<p>So to map event data, one can:</p>

<ul>
  <li>Generate an index:
    <ul>
      <li>Generate all overlapping tuples of <script type="math/tex">d</script> kmers from a reference</li>
      <li>Using a pore model, convert those into points in <script type="math/tex">d</script>-space</li>
      <li>Create a spatial index of those points,</li>
    </ul>
  </li>
  <li>For every read:
    <ul>
      <li>Take every overlapping set of <script type="math/tex">d</script> events</li>
      <li>Look it up in a spatial index</li>
      <li>Find a most likely starting point for the read in the reference, with a quality score.</li>
    </ul>
  </li>
</ul>

<p>Note that one has to explicitly index both the forward and reverse
strands of the reference, since you don’t <em>a priori</em> know
what the “reverse complement” of 65.5 pA is.  One
also has to generate multiple indices; for 2D ONT reads, there is
a pore model for the template strand of a read, and typically two
possible models to choose from for the complement strand of a read,
so one needs three indexes in total to query.</p>

<h3 id="what-dimension-to-choose">What dimension to choose?</h3>

<p>To test this approach, you have to choose a <script type="math/tex">d</script> to use.  On the one
hand, you would like to choose as large a dimension as you can; the
larger <script type="math/tex">k+d-1</script> is, the more unique each seed is and the fewer places
it will occur in any reference sequence.</p>

<p>On the other hand, two quite different constraints put a strong upper limit on the dimensions that will be useful:</p>

<ul>
  <li>The <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a> -
in high spatial dimensions, nearest-neighbour searching is extremely
inefficient, because distance loses its discriminative power.  (Most
things are roughly equally far from each other in 100-dimensional
space; there are a lot of routes you can take!)  As a practical
matter, for most spatial index implementations, for <script type="math/tex">d > 15-20</script> or
so you might as well just do a linear search over all possible
items;</li>
  <li>Current approaches to segmentation mean that ONT data has very
frequent ‘stops’ and ‘skips’ - that is,
events are spuriously either inserted or deleted from the data.
Exactly as with noisy basecalled sequences, this strongly limits
the length of sequence that can be usefully used as seeds.  As we
see below for one set of <em>E. coli</em> data, there is probably not much
point in using <script type="math/tex">d \ge 10</script> even for template-strand data, as the
median “dmer” will have an insertion/deletion in it.</li>
</ul>

<p><img src="/assets/kdtreemapping/events-per-stopskip.png" alt="Distribution of lengths of continuous move events" style="width: 550px; margin-left:auto; margin-right:auto;" /></p>

<p>For these two reasons, we’ve been playing with <script type="math/tex">d \approx
8</script>.  I’ll note that while increasing <script type="math/tex">d</script> is the most obvious
knob to turn to increase specificity of the match, higher <script type="math/tex">k</script>
helps as well.</p>

<h3 id="normalizing-the-signal-levels">Normalizing the signal levels</h3>

<p>One issue we haven’t mentioned is that the raw nanopore data needs
calibration to be compared to the model; there is an additive shift and drift over time,
and a multiplicative scale that has to be taken into account.</p>

<p>A very simple “methods of moments” calculation works
surprisingly well for long-enough reads, certainly well enough to
start an iterative process; for any given model one is trying to
fit a read to, rescaling the mean and standard deviation of read
events to model events gives a good starting point for calibration,
and drift is initially ignored.</p>

<p><img src="/assets/kdtreemapping/initial-rescale.png" alt="Simple initial rescaling of current values by method of moments" /></p>

<h3 id="proof-of-concept">Proof of concept</h3>

<p>A simple proof of concept of using spatial indexing to approximately map squiggle data can be found <a href="https://github.com/ljdursi/simple-squiggle-pseudomapper">on github</a>.  It’s written in python, and has <code class="highlighter-rouge">scipy</code>, <code class="highlighter-rouge">h5py</code>, and <code class="highlighter-rouge">matplotlib</code> as dependencies.  Note that as implemented, it is absurdly memory-hungry, and definitely requires a numpy and scipy built against a good BLAS implementation.</p>

<p>As a spatial index, it uses a version of a <a href="https://en.wikipedia.org/wiki/K-d_tree">k-d tree</a> (<code class="highlighter-rouge">scipy.spatial.cKDTree</code>), which is a very versatile and widely used (and so well-optimized) spatial index widely used in machine learning methods amongst others; different structures may have advantages for this application.</p>

<p>Running the <code class="highlighter-rouge">index-and-map.sh</code> script generates an index for the provided <code class="highlighter-rouge">ecoli.fa</code> reference - about 1 minute per pore model - and then maps the 10 reads provided of both older 5mer and newer 6mer MAP data.  Mapping each read takes about 6 seconds per read per pore model; this involves lots of python list manipulations so could fairly straightforwardly be made much faster.<br />
Doing the simplest thing possible for mapping works surprisingly well.  Using the same sort of approach as the first steps of the Sovic <em>et al.</em> method<sup id="fnref:1:1"><a href="#fn:1" class="footnote">1</a></sup>, we just:</p>

<ul>
  <li>Use the default k-d tree parameters (which almost certainly isn’t right, particularly the distance measure)</li>
  <li>Consider overlapping bins of starting positions on the reference, of size ~10,000 events, a typical read size</li>
  <li>For each <script type="math/tex">d</script>-point in the read,
    <ul>
      <li>Take the closest match to each <script type="math/tex">d</script>-point (or all within some distance)</li>
      <li>For each match, add a score to the bin corresponding to the implied starting position of the read on the reference; a higher score for a closer match</li>
    </ul>
  </li>
  <li>Report the best match starting point</li>
</ul>

<p>Let’s take a look at the initial output for the older SQK005 ecoli data and newer SQK006 data:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./index-and-map.sh

5mer data: 

Indexing : model models/5mer/template.model, dimension 8
python spatialindex.py --dimension 8 ecoli.fa models/5mer/template.model indices/ecoli-5mer-template

real	0m52.922s
user	0m49.756s
sys	0m1.196s
Mapping reads: starting with ecoli/005/LomanLabz_PC_Ecoli_K12_R7.3_2549_1_ch182_file148_strand.fast5
<span class="nb">time </span>python ./mapread.py --plot save --plotdir plots --closest    --maxdist 3.5  <span class="se">\</span>
    --templateindex indices/ecoli-5mer-template.kdtidx    
    ecoli/005/LomanLabz_PC_Ecoli_K12_R7.3_2549_1_ch182_file148_strand.fast5 ...  &gt; template-005-.txt

real	1m9.776s
user	1m9.108s
sys	0m1.004s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch277_file143  819         3079177  3079996  17.164000
ch498_file171  1130        3793866  3794996  11.916000
ch222_file28   1765        2803231  2804996  11.152000
ch461_file9    1895        2195243  2193348  8.749000
ch182_file148  2559        2767555  2764996  17.120000
ch34_file53    4772        1128120  1123348  11.235000
ch401_file98   4981        2328329  2323348  6.160000
ch80_file64    5649        4369347  4374996  7.469000
ch395_file89   5828        2069168  2074996  9.517000
ch464_file15   7031        2195379  2188348  14.782000
Indexing : model models/5mer/complement.model, dimension 8
python spatialindex.py --dimension 8 ecoli.fa models/5mer/complement.model indices/ecoli-5mer-complement

real	0m52.124s
user	0m47.948s
sys	0m1.332s

Mapping reads: starting with ecoli/005/LomanLabz_PC_Ecoli_K12_R7.3_2549_1_ch182_file148_strand.fast5
<span class="nb">time </span>python ./mapread.py --plot save --plotdir plots --closest    --maxdist 3.5  <span class="se">\</span>
    --templateindex indices/ecoli-5mer-template.kdtidx  <span class="se">\</span>
    --complementindex indices/ecoli-5mer-complement.kdtidx <span class="se">\</span>
    ecoli/005/LomanLabz_PC_Ecoli_K12_R7.3_2549_1_ch182_file148_strand.fast5 ...  <span class="se">\</span>
    &gt; template-complement-005-.txt

real	1m47.590s
user	1m46.228s
sys	0m1.712s

Template+Complement Alignements
Read           Difference  BWA      KDTree   zscore
ch401_file98   19          2328329  2328348  7.516000
ch277_file143  819         3079177  3079996  17.164000
ch498_file171  1130        3793866  3794996  11.916000
ch222_file28   1765        2803231  2804996  11.152000
ch461_file9    1895        2195243  2193348  10.378000
ch182_file148  2559        2767555  2764996  17.120000
ch34_file53    4772        1128120  1123348  11.235000
ch80_file64    5649        4369347  4374996  7.469000
ch395_file89   5828        2069168  2074996  9.517000
ch464_file15   7031        2195379  2188348  14.782000

6mer data: 

Indexing : model models/6mer/template.model, dimension 8
python spatialindex.py --dimension 8 ecoli.fa models/6mer/template.model indices/ecoli-6mer-template

real	1m4.028s
user	0m57.924s
sys	0m1.872s

Mapping reads: starting with ecoli/006/LomanLabz_PC_Ecoli_K12_MG1655_20150924_MAP006_1_5005_1_ch102_file146_strand.fast5
<span class="nb">time </span>python ./mapread.py --plot save --plotdir plots --closest --maxdist 3.5 <span class="se">\</span>
    --templateindex indices/ecoli-6mer-template.kdtidx <span class="se">\</span>
    ecoli/006/LomanLabz_PC_Ecoli_K12_MG1655_20150924_MAP006_1_5005_1_ch102_file146_strand.fast5 ...  &gt; template-006-.txt

real	1m46.643s
user	1m45.636s
sys	0m1.276s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch383_file65   844         89152    89996    20.490000
ch209_file0    1119        4564467  4563348  24.038000
ch485_file6    1739        400087   398348   23.698000
ch179_file92   3113        496461   493348   16.477000
ch102_file146  4965        1378313  1373348  25.003000
ch241_file2    5625        1578973  1573348  5.919000
ch339_file119  5710        829058   823348   22.348000
ch412_file79   5806        2194190  2199996  19.218000
ch228_file128  5933        3809063  3814996  17.926000
ch206_file36   11173       4373823  4384996  28.011000

Indexing : model models/6mer/complement_pop1.model, dimension 8
python spatialindex.py --dimension 8 ecoli.fa models/6mer/complement_pop1.model indices/ecoli-6mer-complement_pop1
python spatialindex.py --dimension 8 ecoli.fa models/6mer/complement_pop2.model indices/ecoli-6mer-complement_pop2
<span class="o">[</span>..]

Mapping reads: starting with ecoli/006/LomanLabz_PC_Ecoli_K12_MG1655_20150924_MAP006_1_5005_1_ch102_file146_strand.fast5
<span class="nb">time </span>python ./mapread.py --plot save --plotdir plots --closest --maxdist 3.5  <span class="se">\</span>
    --templateindex indices/ecoli-6mer-template.kdtidx  <span class="se">\</span>
    --complementindex indices/ecoli-6mer-complement_pop1.kdtidx,indices/ecoli-6mer-complement_pop2.kdtidx <span class="se">\</span>
    ecoli/006/LomanLabz_PC_Ecoli_K12_MG1655_20150924_MAP006_1_5005_1_ch102_file146_strand.fast5 ...  &gt; template-complement-006-.txt

real	3m55.773s
user	3m31.032s
sys	0m4.364s

Template+Complement Alignements
Read           Difference  BWA      KDTree   zscore
ch383_file65   844         89152    89996    20.490000
ch209_file0    1119        4564467  4563348  24.038000
ch485_file6    1739        400087   398348   23.698000
ch179_file92   3113        496461   493348   16.477000
ch102_file146  4965        1378313  1373348  25.003000
ch339_file119  5710        829058   823348   22.348000
ch412_file79   5806        2194190  2199996  19.218000
ch228_file128  5933        3809063  3814996  17.926000
ch241_file2    9375        1578973  1588348  11.626000
ch206_file36   11173       4373823  4384996  28.011000</code></pre></figure>

<p>We see a couple of things here:</p>

<ul>
  <li>Adding the complement strand does almost nothing for the accuracy,
but requires substantially more memory and compute time, as multiple
indices must be loaded and used up, and all candidate complement
indices must be compared against each other.  Because of this and
the typically higher skip/stay rates for complement strands, we will use the template
strand only for the rest of this post;</li>
  <li>Since we are simply assigning starting bins at this point, 
any assignments within the bin size are equally accurate; here, all
of the reads were correctly assigned to the starting bin or the neighbouring one.</li>
  <li>The newer 6mer data gives slightly better results; part of this
is likely because we are using the same <script type="math/tex">d</script>, so a dmer for the k=6 data
corresponds to seed longer by one</li>
  <li>The zscore here is a very crude measure of how much the assignment
stands out over the background (but not necessarily how it compares
to other candidate mappings); some of these very simple pseudo-mappings 
are relatively securely identified, and others less so.  The sum-of-scores for
the reads with the best and worst zscore results are plotted below; no prize for
guessing which is which:</li>
</ul>

<table>
<tr>
<td> 5mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch277_file143__simple.png" alt="ch277_file143 simple" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch401_file98__simple.png" alt="ch401_file98 simple" style="width: 400px;" /> </td>
</tr>
<tr>
<td> 6mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch206_file36__simple.png" alt="ch206_file36 simple" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch241_file2__simple.png" alt="ch241_file2 simple" style="width: 400px;" /> </td>
</tr>
</table>

<p>Because many levels are clustered near ~60-70pA, many dmers are quite
close to each other, and choosing simply the closest <script type="math/tex">d</script>-point is unlikely
to give a robust result.  Examining all possible matches in the spatial
index within some given radius reduces the noise somewhat, at a modest
increase in compute time:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./index-and-map.sh noclosest templateonly
5mer data: 
<span class="o">[</span>...]
real	3m15.145s
user	3m5.840s
sys	0m2.972s

Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch401_file98   19          2328329  2328348  11.359000
ch277_file143  819         3079177  3079996  19.103000
ch498_file171  1130        3793866  3794996  15.353000
ch222_file28   1765        2803231  2804996  17.583000
ch182_file148  2559        2767555  2764996  18.579000
ch34_file53    4772        1128120  1123348  16.615000
ch80_file64    5649        4369347  4374996  10.181000
ch395_file89   5828        2069168  2074996  13.145000
ch461_file9    6895        2195243  2188348  10.431000
ch464_file15   7031        2195379  2188348  18.042000

6mer data: 
<span class="o">[</span>...]

real	9m29.562s
user	9m9.536s
sys	0m16.600s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch383_file65   844         89152    89996    21.777000
ch485_file6    1739        400087   398348   24.384000
ch179_file92   3113        496461   493348   19.718000
ch102_file146  4965        1378313  1373348  24.218000
ch241_file2    5625        1578973  1573348  7.311000
ch339_file119  5710        829058   823348   26.045000
ch228_file128  5933        3809063  3814996  20.042000
ch209_file0    6119        4564467  4558348  23.703000
ch412_file79   10806       2194190  2204996  21.677000
ch206_file36   11173       4373823  4384996  29.794000</code></pre></figure>

<p>Note the increase in zscores; the same two reads are plotted:</p>

<table>
<tr>
<td> 5mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch277_file143__noclosest.png" alt="ch277_file143 noclosest" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch401_file98__noclosest.png" alt="ch401_file98 noclosest" style="width: 400px;" /> </td>
</tr>
<tr>
<td> 6mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch206_file36__noclosest.png" alt="ch206_file36 noclosest" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch241_file2__noclosest.png" alt="ch241_file2 noclosest" style="width: 400px;" /> </td>
</tr>
</table>

<p>With a few other tweaks - keeping track of the top 10 candidates,
and for each re-testing by recalibrating given the inferred mapping
and rescoring - we can get about 95%-99% accuracy in mapping.</p>

<p><img src="/assets/kdtreemapping/dotplot.png" alt="kd-tree approximate mapping vs BWA MEM mapping positions" /></p>

<p>Of course, while 95-99% (Q13-Q20) mapping accuracy on <em>E. coli</em> is
a cute outcome from such a simple approach, it isn’t nearly
enough; with <script type="math/tex">d=8</script> and <script type="math/tex">k=5</script>, wer’e working with seeds of
size 12, which would typically be unique in the <em>E. coli</em> reference,
but certainly wouldn’t be in the human genome, or for metagenomic
applications.</p>

<h3 id="em-rescaling">EM Rescaling</h3>

<p>One limiting factor is the approximate nature of the rescaling so
far that is being performed; for reads that have been basecalled,
the inferred shift values above can be off by several picoamps from
the Metrichor values, which clearly causes both false negatives and
false positives in the index lookup.  This can be addressed by doing
a more careful rescaling step, using EM iterations:</p>

<ul>
  <li>For the E-step, provisionally assign probabilities of read events
corresponding to model levels, based on the Gaussian distributions 
described in the model and a simple transition matrix between events;</li>
  <li>For the M-step, perform a weighted least squares regression to
re-scale the read levels.</li>
</ul>

<p>This gives answers that are quite good when compared with Metrichor,
at the cost of substantially more computational effort (much greater
than the spatial index lookup!), particularly for long reads and
larger (k) where the number of model levels is larger:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./index-and-map.sh noclosest templateonly rescale
5mer data: 
<span class="o">[</span>...]
real	7m1.866s
user	6m56.708s
sys	0m29.236s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch401_file98   19          2328329  2328348  14.975000
ch277_file143  819         3079177  3079996  22.516000
ch498_file171  1130        3793866  3794996  20.894000
ch222_file28   1765        2803231  2804996  19.569000
ch182_file148  2559        2767555  2764996  20.443000
ch34_file53    4772        1128120  1123348  17.401000
ch395_file89   5828        2069168  2074996  13.510000
ch461_file9    6895        2195243  2188348  11.165000
ch464_file15   7031        2195379  2188348  19.315000
ch80_file64    10649       4369347  4379996  15.109000

6mer data: 
<span class="o">[</span>...]
real	33m11.324s
user	31m55.172s
sys	2m0.836s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch383_file65   844         89152    89996    23.732000
ch179_file92   3113        496461   493348   20.845000
ch102_file146  4965        1378313  1373348  24.997000
ch241_file2    5625        1578973  1573348  10.574000
ch339_file119  5710        829058   823348   26.816000
ch228_file128  5933        3809063  3814996  21.397000
ch209_file0    6119        4564467  4558348  24.546000
ch485_file6    6739        400087   393348   24.790000
ch412_file79   10806       2194190  2204996  25.477000
ch206_file36   11173       4373823  4384996  30.164000</code></pre></figure>

<p>Note again the increase in zscores; replotting gives:</p>

<table>
<tr>
<td> 5mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch277_file143__rescale.png" alt="ch277_file143 rescale" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch401_file98__rescale.png" alt="ch401_file98 rescale" style="width: 400px;" /> </td>
</tr>
<tr>
<td> 6mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch206_file36__rescale.png" alt="ch206_file36 rescale" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch241_file2__rescale.png" alt="ch241_file2 rescale" style="width: 400px;" /> </td>
</tr>
</table>

<h3 id="extending-the-seeds">Extending the seeds</h3>

<p>So far we have in no way taken into account any of the locality
information in the spatial index lookup results; that is, that a long
series of hits close together, in the same order on the read and on the
reference, is much stronger evidence for a good mapping than a haphazard
series of hits in random order that happen to fall within the same bin
of starting points.</p>

<p>Keeping with the do-the-simplest-thing approach that has worked so
far, we can try to extend these “seed” matches by stitching them
together into longer seeds; here we build 15-mers out of sets of 4
neighbouring 12-mers, allowing one skip or stay somewhere within
them, using as a score for the result the minimum of the constituent
scores, and dropping all hits that cannot be so extended.  This has
quite modest additional cost, and works quite well:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./index-and-map.sh noclosest templateonly rescale extend
5mer data: 

<span class="o">[</span>...]
real	7m20.914s
user	7m19.228s
sys	0m23.620s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch498_file171  1130        3793866  3794996  28.762000
ch222_file28   1765        2803231  2804996  27.434000
ch461_file9    1895        2195243  2193348  29.089000
ch182_file148  2559        2767555  2764996  29.538000
ch34_file53    4772        1128120  1123348  28.837000
ch401_file98   4981        2328329  2323348  23.840000
ch277_file143  5819        3079177  3084996  29.832000
ch395_file89   5828        2069168  2074996  23.705000
ch464_file15   7031        2195379  2188348  28.941000
ch80_file64    10649       4369347  4379996  26.926000

6mer data: 
<span class="o">[</span>...]
real	33m57.863s
user	32m58.236s
sys	1m42.548s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch241_file2    625         1578973  1578348  25.137000
ch485_file6    1739        400087   398348   30.297000
ch339_file119  5710        829058   823348   31.856000
ch383_file65   5844        89152    94996    30.163000
ch228_file128  5933        3809063  3814996  29.294000
ch209_file0    6119        4564467  4558348  30.169000
ch179_file92   8113        496461   488348   28.732000
ch102_file146  9965        1378313  1368348  30.107000
ch412_file79   10806       2194190  2204996  30.377000
ch206_file36   11173       4373823  4384996  33.896000</code></pre></figure>

<table>
<tr>
<td> 5mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch277_file143__extend.png" alt="ch277_file143 extend" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch401_file98__extend.png" alt="ch401_file98 extend" style="width: 400px;" /> </td>
</tr>
<tr>
<td> 6mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch206_file36__extend.png" alt="ch206_file36 extend" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch241_file2__extend.png" alt="ch241_file2 extend" style="width: 400px;" /> </td>
</tr>
</table>

<p>Now that we seem to have much stronger and more specific results,
we can investigate letting go of binning, and instead string these
extended seeds into longest possible matches.  Continuing to extend
dmer-by-dmer is too challenging due to missing points (due to
mis-calibration, too large noise in the dmer falling out of the
maxdist window in the kdtree, or several skip/stays in a row), so
following the ideas of graphmap<sup id="fnref:1:2"><a href="#fn:1" class="footnote">1</a></sup> and minimap<sup id="fnref:5:1"><a href="#fn:5" class="footnote">5</a></sup>, we trace
collinear seeds through the set of available seeds; rather than
just taking longest increasing sequences in inferred start positions,
however, we make a graph of seeds with edges connecting  seeds that
are monotonically increasing both in read location and reference
location with ‘small enough’ jumps, and extract longest paths through
this graph.  The cost of this is smaller than the rescaling of the read.</p>

<p>Running this, the zscores now somewhat change meaning; only the extracted
paths are scored, meaning that the scores are only amongst plausible
mappings, not over all bins across the reference.  Similarly, the differences
in mapping locations are somewhat more meaningful - rather than being compared
to bin centres, they are actually the differences between mapping locations.</p>

<p>We get:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">5mer data: 
<span class="o">[</span>...]

real	6m54.776s
user	6m52.064s
sys	0m23.648s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch461_file9    10          2195243  2195253  7.105000
ch34_file53    343         1128120  1128463  12.449000
ch401_file98   363         2328329  2328692  5.320000
ch464_file15   2826        2195379  2192553  13.267000
ch498_file171  3192        3793866  3797058  7.688000
ch182_file148  5450        2767555  2773005  10.107000
ch222_file28   5681        2803231  2808912  8.166000
ch277_file143  6381        3079177  3085558  9.190000
ch395_file89   7191        2069168  2076359  7.685000
ch80_file64    15167       4369347  4384514  7.965000


6mer data: 
<span class="o">[</span>...]

real	43m37.651s
user	42m37.208s
sys	1m42.848s
Template Only Alignments
Read           Difference  BWA      KDTree   zscore
ch209_file0    14          4564467  4564453  31.724000
ch485_file6    14          400087   400073   24.063000
ch241_file2    203         1578973  1578770  26.676000
ch179_file92   723         496461   495738   39.626000
ch339_file119  774         829058   828284   45.636000
ch102_file146  1568        1378313  1376745  45.601000
ch383_file65   5890        89152    95042    27.902000
ch228_file128  8120        3809063  3817183  35.863000
ch412_file79   13833       2194190  2208023  48.444000
ch206_file36   15650       4373823  4389473  38.441000</code></pre></figure>

<table>
<tr>
<td> 5mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch277_file143__longest.png" alt="ch277_file143 longest" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch401_file98__longest.png" alt="ch401_file98 longest" style="width: 400px;" /> </td>
</tr>
<tr>
<td> 6mer </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch206_file36__longest.png" alt="ch206_file36 longest" style="width: 400px;" /> </td>
<td> <img src="/assets/kdtreemapping/saveplots/ch241_file2__longest.png" alt="ch241_file2 longest" style="width: 400px;" /> </td>
</tr>
</table>

<p>The simple python testbed implementation of these ideas linked to above
is very slow, single-threaded, absurdly memory hungry, and its treatment
of scores for the mappings does not currently make much sense.  In
the new year we will address these issues in a proper C++ implementation,
where (for instance) we will not have multiple copies of large, reference-sized
data structures.</p>

<hr />

<h3 id="references">References</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://biorxiv.org/content/early/2015/06/10/020719">Fast and sensitive mapping of error-prone nanopore sequencing reads with GraphMap</a> (2015) by Sovic, Sikic, Wilm, <em>et al.</em> <a href="#fnref:1" class="reversefootnote">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:2">
      <p><a href="http://arxiv.org/abs/1505.02710">Near-optimal RNA-Seq quantification</a> (2015) by Bray, Pimentel, Melsted, and Pachter <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="http://biorxiv.org/content/early/2015/06/27/021592">Salmon: Accurate, Versatile and Ultrafast Quantification from RNA-seq Data using Lightweight-Alignment</a> (2015) by Patro, Duggal, and Kingsford <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://github.com/COMBINE-lab/RapMap">RapMap</a>, COMBINE lab <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p><a href="http://arxiv.org/abs/1512.01801">Minimap and miniasm: fast mapping and denovo assembly for noisy long sequences</a>, Heng Li <a href="#fnref:5" class="reversefootnote">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>
</content>
   </entry>
  
 
  
   <entry>
     <title>Nanopolish v0.4.0</title>
     <link href="http://simpsonlab.github.io/2015/10/07/nanopolish-v0.4.0/"/>
     <updated>2015-10-07T00:00:00-07:00</updated>
     <id>simpsonlab.github.io/2015/10/07/nanopolish-v0.4.0</id>
     <content type="html"><p>This is a long overdue post describing recent changes to <a href="https://github.com/jts/nanopolish">nanopolish</a>.</p>

<h3 id="sqk006-support">SQK006 Support</h3>

<p>Last month Oxford Nanopore released a new sequencing kit, SQK006. As Nick <a href="http://lab.loman.net/2015/09/24/first-sqk-map-006-experiment/">notes</a> there are two important changes that affect how we model the data. Most importantly, ONT changed their basecaller to use 6-mers rather than 5-mers. The relationship between the DNA sequence in the pore and the measured signals is not simple (to understate the problem). By moving to a signal model with longer context the effect of long-range interactions that are not captured by shorter k-mer is hopefully reduced, which may improve read accuracy. In nanopolish v0.4.0 we now support the 6-mer model. This required reworking our data structures to allow either a 5-mer or 6-mer model depending on what sequencing kit was used to generate the data.</p>

<p>SQK006 also brings an increase in speed - from 30bp/s (SQK005) to 70bp/s (SQK006). With the sampling rate fixed to 3kHz we naturally expect more events to be missed by the event detector. We observed this in our analysis of SQK006 data and updated the initial transition probabilities in our HMM accordingly.</p>

<h4 id="sqk006-accuracy">SQK006 Accuracy</h4>

<p>We were quite curious to see how well the new data performs in practice. Nick and Josh made four runs with SQK006 kits - two of native E. coli DNA and two of PCR-treated E. coli DNA to remove DNA damage, base modifications and other artefacts. I downloaded one of the native runs and one of the PCR-treated runs and used it to make a new consensus sequence for the draft assembly in our <a href="http://www.nature.com/nmeth/journal/v12/n8/full/nmeth.3444.html">paper</a>. As before we use dnadiff from <a href="http://mummer.sourceforge.net/">mummer</a> to calculate percent identity, the number of SNPs and the number of indels with respect to the E. coli K12 reference genome.</p>

<table>
  <thead>
    <tr>
      <th>Kit, Coverage</th>
      <th>Percent Identity</th>
      <th># SNPs</th>
      <th># Indels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SQK005, 29X</td>
      <td>99.48%</td>
      <td>1,343</td>
      <td>22,601</td>
    </tr>
    <tr>
      <td>SQK006, 48X</td>
      <td>99.78%</td>
      <td>644</td>
      <td>9,697</td>
    </tr>
    <tr>
      <td>SQK006-PCR, 30X</td>
      <td>99.82%</td>
      <td>222</td>
      <td>8,200</td>
    </tr>
  </tbody>
</table>

<p>The accuracy of our assembly is much better with SQK006 data. Most of this improvement is due to better representation of homopolymer sequences. Interestingly the PCR-treated run gave the best results despite being lower coverage than the native DNA run. It is tempting to attribute this to providing cleaner DNA to the nanopore but this requires further exploration first.</p>

<h3 id="eventalign">Eventalign</h3>

<p>In a previous post I introduced the <a href="http://simpsonlab.github.io/2015/04/08/eventalign/">eventalign</a> submodule, which uses the hidden Markov model to align events to a reference genome. In nanopolish v0.4.0 there are a few eventalign changes and new features. The bulky <code class="highlighter-rouge">model_name</code> field has been removed from the output - it bloated the file as every row for a particular read would contain the same information. This read-level information has been moved to a secondary output file which can be enabled with the <code class="highlighter-rouge">--summary FILE</code> option. The summary file contains the path to the FAST5 file and model name for each read, along with the number of aligned events and other metadata that may be useful for exploring the alignments.</p>

<h4 id="experimental-sam-output">Experimental SAM output</h4>

<p>eventalign now has an optional <code class="highlighter-rouge">--sam</code> flag which will write the alignment in a modified version of the SAM format. By example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>example_2d_read.template    0   chr1    10001   60  9950S4M1I1M2I1M1... *   0   0   *   * ES:i:1
</code></pre>
</div>

<p>Most of these fields should be familiar - the alignment contains a read name (annotated with the sequenced strand) followed by flags, a reference name and the alignment start position. Similar to a base-to-base alignment the CIGAR string describes the alignment of events to reference k-mers. <code class="highlighter-rouge">9950S4M</code> indicates <code class="highlighter-rouge">9950</code> events should be skipped. The next event, <code class="highlighter-rouge">9951</code>, aligns to the k-mer starting at reference position <code class="highlighter-rouge">10001</code>, followed by three consecutive matches and so on. Sequence and quality information is not stored. The <code class="highlighter-rouge">ES</code> auxiliary tag is the <em>event stride</em> - whether the event indices are increasing along the reference sequence (<code class="highlighter-rouge">ES:i:1</code>) or whether they are decreasing (<code class="highlighter-rouge">ES:i:-1</code>). The nanopolish parser for this format is <a href="https://github.com/jts/nanopolish/blob/master/src/alignment/nanopolish_alignment_db.cpp#L347">here</a>.</p>

<p>There are a few reasons for using this format. First, it is much smaller than the eventalign tsv output. Second, by adopting sam/bam instead of creating a new binary format we can use the existing <code class="highlighter-rouge">samtools/htslib</code> ecosystem. Crucially, we can use <code class="highlighter-rouge">htslib</code> to provide random access into the alignments for an indexed bam file. This format was developed through discussions with Vadim Zalunin, Ewan Birney, Mick Watson and others. Comments welcome.</p>
</content>
   </entry>
  
 
  
   <entry>
     <title>Merging Structural Variant Calls from Different Callers</title>
     <link href="http://simpsonlab.github.io/2015/06/15/merging-sv-calls/"/>
     <updated>2015-06-15T00:00:00-07:00</updated>
     <id>simpsonlab.github.io/2015/06/15/merging-sv-calls</id>
     <content type="html"><p>As part of the work of the <a href="http://pancancer.info/variant_calling.html">Pancancer variant-calling working
group</a>, we needed to merge the
results of variant calls from a wide range of different packages to compare
their results and select interesting sites for lab validation.  This is a 
more subtle procedure than it sounds, and we could not find any one 
place where all the necessary information was documented, so we wrote up 
our process here.  Code that implements this part of our analysis pipeline
can be found on <a href="https://github.com/ljdursi/mergevcf">GitHub</a>.</p>

<h2 id="simple-variants---snvs-indels">Simple Variants - SNVs, Indels</h2>

<p>The standard format used to output variants is the <a href="http://en.wikipedia.org/wiki/Variant_Call_Format">Variant Call Format</a>.  For <a href="http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism">SNVs</a> and shortish <a href="http://en.wikipedia.org/wiki/Indel">indels</a> (insertions or deletions), this works very well.  Each entry in a VCF file contains the location of the variant (the chromosome it occurs on, and the starting position), the relevant excerpt of the reference sequence starting at that position, and the “alternate” sequence – the variant sequence that has been found there instead.</p>

<p>There are other fields that we will come back to; but a VCF file
containing an A → G SNV at chromosome 1, position 100, a
3 base-pair deletion at chromosome 2, position 200, and a 5 basepair
insertion at chromosome 3, position 300 would look something like this:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#CHROM POS  ID  REF     ALT     ..</span>
1       100 .   A       G   
2       200 .   CGTA    C
3       300 .   T       TACGTA</code></pre></figure>

<p>Even this admits some ambiguity; for instance, a deletion of 3 As within a
homopolymer run of 10 of them could reasonably be called at any of 8 positions;
and more complex substitions can be equally described as one large variant, or
as a combination of insertions, deletions, and substitions.  To break these
ambiguities, there is a well understood
<a href="http://genome.sph.umich.edu/wiki/Variant_Normalization">normalization</a>
process, which requires looking at both the reference genome and the VCF file.
It is fairly straightforward, implemented by several
tools, and we perform this step (using <a href="https://samtools.github.io/bcftools/bcftools.html#norm">bcftools
norm</a>) upon
ingesting the submitted calls from various groups.</p>

<p>Once these sorts of calls are normalized, they are fairly easily merged and compared.  We used python for this project, and we used Jared’s code from an earlier pilot for this – using a dictionary of dictionaries, where the first key was a (chromosome,start position) tuple.  The value in the first dictionary corresponding to that key was then a dictionary of (reference allele, variant allele), with a value that was a list of callers that had made that call.  So querying existance of a variant was just two (at most) dictionary lookups; and registering that a caller made a particular call was two dictionary lookups and a list append, possibly creating dictionaries and lists along the way if this was the first time these entries were seen.</p>

<h2 id="structural-variants">Structural Variants</h2>

<p>Performing the same task with structural variants (henceforth SVs) was more complicated for two reasons:</p>

<ol>
  <li>Location - by nature of how SVs are detected, there is quite often uncertainty in the position of called SVs, making comparison more complicated; and</li>
  <li>Description - there are a large number of ways that callers actually call SVs, and they need to be converted to some common representation for comparison.</li>
</ol>

<h3 id="location">Location</h3>

<p>Structural variants are generally observed differently than, say, SNVs.  An alignment-based method might align a read to a reference, find that the alignment unambigiously implies a mismatch, and so call an SNV, as in the top panel of the figure below; there is no question about the location of the variant.   (Obviously, to call the variant with any confidence would require multiple reads, not shown in the figure for clarity.)</p>

<p><img src="/assets/sv/mapping-alpha.png" alt="Mapping SNVs vs SVs" /></p>

<p>However, a structural variant – say a translocation, as in the bottom panel of the figure – will likely be found differently.  In the diagram, paired ends of a read map to two different chromosomes unambigiously, but there is ambiguity as to the location of the two breakpoints on the chromosomes which are now joined to form an new adjacency.   Information from other reads will reduce the uncertainties, even to zero; but in the general case one has to handle ambiguity in location when deciding which calls to merge.</p>

<p>Many callers provided confidence intervals on locations of the breakpoints in their calls, which could be used for constraining the merging of calls.  However, not all callers did; and there was, naturally enough, significant variation in the confidence intervals, even for a given variant.  Thus we took a simpler approach, merging using a fixed window size. Two calls, where each breakpoint of one call was less than the window size away from the corresponding breakpoint of the other call, were merged.</p>

<p>We used a fixed window size of 300bp, of the order of the insert size observed
in the libraries; in principle, this is a more agressive merging strategy than using the confidence intervals when available (which were typically much less), but in practice, there were very few pairs of calls where the larger window size would have mattered.</p>

<p>The most efficient way to look up possible matching locations within some
window would be to use a spatial data structure such as an <a href="http://en.wikipedia.org/wiki/Interval_tree">interval
tree</a>; however, since our window
size was relatively modest, and we are constrained to integer positions, I
dealt with uncertainty in locations in location lookups by brute-force; I
simply search every integer position within the window.  For efficiency, and to
ensure the closest match is found, the search is by distance (pos+0, pos±1, …).  Because dictionary lookup in python is so fast, this approach is “fast enough” for our purposes, in that this is not the rate limiting step for the merging pipeline.</p>

<h3 id="description">Description</h3>

<p>A bigger challenge – not conceptually, but in terms of bookkeeping and corner cases – is simply the diversity of ways in which calls are labelled in VCF files by various callers.</p>

<h4 id="breakpoint-notation-svtypebnd">Breakpoint notation (SVTYPE=BND)</h4>

<p>The <a href="http://samtools.github.io/hts-specs/">VCF Standard</a> describes two ways of describing SVs in a VCF file, of which there are numerous small variations out in the field.  The second, described in somewhat more detail, is breakend notation (usually labelled with an SVTYPE=BND entry in the INFO field of a record).  Before we go into this in much detail, let’s look at a figure describing the possibilties:</p>

<p><img src="/assets/sv/bkpts-alpha.png" alt="VCF Breakpoint meanings" /></p>

<p>To describe a structural variant, we need more than just two positions, one per breakend.  The breakends aren’t just points; they can be thought of as half-intervals, (eg, the piece of Chromosome 1 leading up to position 500; or the piece of Chromsome 1 starting at and continuing from position 500), and we will need that directional information.</p>

<p>(In the VCF spec, from the diagrams it is pretty clear that these are closed intervals; <em>eg</em>, regardless of the direction of the half-interval, the interval includes position 500.  It is not 100% clear to me that every caller that uses BND notation honours that convention, but since we’re using 300-bp windows, such off-by-one errors in location need not concern us here.).</p>

<p>Once that is established, there is one more piece of information which need to be established to define the adjacency between breakends A and B; it can be thought of in two equivalent ways.  One is the relative orientation; is half-interval A joined after half-interval B, or before?   The second is the (relative) strandedness.  Are the two half-intervals joined in such a way that the same strands are joined, or are the strands reversed, as in an inversion?</p>

<p>In VCF BND notation, for consistency with other entries but perhaps somewhat confusingly, only the chromosome and position of the first breakpoint are listed, and then in the ALT field is encoded:</p>

<ul>
  <li>The other chromosome and position of the other breakend (eg, 1:800 in the diagrams above)</li>
  <li>The orientation of the second breakend; whether this is the half-interval which starts at the given position and extends rightwards, <code class="highlighter-rouge">[1:800[</code>, or extends from the left and ends at the position given, <code class="highlighter-rouge">]1:800]</code>.</li>
  <li>Finally, the relative orientation of the first breakpoint and its direction is specified using the position of other bases relative to the second breakpoint.
    <ul>
      <li>In the examples in the figure, ‘N’ is given in the REF field; the position of that N relative to the <code class="highlighter-rouge">]1:800]</code> or <code class="highlighter-rouge">[1:800[</code> tells you whether the breakpoint involving 1:500 is joined before or after.</li>
      <li>If after, it is the half-interval including that position and extending rightwards, other it is the breakpoint including that position extending to the left.</li>
      <li>That combined with the relative orientation of the second interval is enough to establish the adjacency.</li>
      <li>The <code class="highlighter-rouge">N</code>, of course, could be a real base, if known; and the corresponding bases listed in the ALT field could be one or more different bases, as would be the case if a substitution occurred at or insertion occured between the two half-intervals being joined at the new adjacency.</li>
    </ul>
  </li>
</ul>

<p>Note that the adjacency can be described equally well from either side; that is, for the purposes of identifying the new adjacency we could just as easily list 1:800 first:</p>

<table>
  <thead>
    <tr>
      <th>500 first</th>
      <th>800 first</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   N[1:800[</code></td>
      <td><code class="highlighter-rouge">1    800 .   N   ]1:500]N</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   ]1:800]N</code></td>
      <td><code class="highlighter-rouge">1    800 .   N   N[1:500[</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   [1:800[N</code></td>
      <td><code class="highlighter-rouge">1    800 .   N   [1:500[N</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   N]1:800]</code></td>
      <td><code class="highlighter-rouge">1    800 .   N   N]1:500]</code></td>
    </tr>
  </tbody>
</table>

<p>When flipping the order of the breakpoints, we either do nothing else (in the case of adjacencies with different relative strandedness/orientation) or also flip position and direction (in the case of adjacencies with the same relative orientation).  Otherwise, we end up describing a different novel adjacency involving the same two positions.</p>

<p>Because we want to merge equivalent adjacencies, and some callers call an adjacency from both sides and others just from one, we normalize the order by always listing the lexicographically first breakpoint position first.</p>

<h4 id="symbolic-notation-translocations-tra">Symbolic Notation Translocations (&lt;TRA&gt;)</h4>

<p>The other way given for describing a structural variant is with “symbolic notation”; listing meaningful tags like &lt;INV&gt; to describe an inversion, or &lt;DUP&gt; to describe a duplication, in the ALT field of a VCF record.</p>

<p>The most general of these is the translocation symbolic call, &lt;TRA&gt;, which isn’t covered in the VCF standard, which essentially works the same way as a BND-style call.  However, since the ALT field is now occupied, we need some other way to put the information about the position of the second breakpoint, and the relative orientation of the two half-intervals.</p>

<p>The other position is listed in the INFO field, with the chromosome given in an entry called CHR2, and the position in an entry called END.   That leaves the relative orientation with which they are joined.</p>

<p>Although again this doesn’t appear to be covered in the VCF specification, the
convention is to use a “connection” field, CT, to indicate whether the 5’ or the 3’ end of the interval involving the first breakpoint position is connected to the 5’ or 3’ end of that involving the second.  This gives the directions of both half-intervals and their connection, which is enough to define the adjacency.</p>

<p>We can then translate between BND notation and translocation calls:</p>

<table>
  <thead>
    <tr>
      <th>BND</th>
      <th>&lt;TRA&gt; with CT INFO field</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   N[1:800[</code></td>
      <td><code class="highlighter-rouge">1    500 .   N   &lt;TRA&gt;   ... CHR2=1;END=800;CT='3to5'</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   ]1:800]N</code></td>
      <td><code class="highlighter-rouge">1    500 .   N   &lt;TRA&gt;   ... CHR2=1;END=800;CT='5to3'</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   [1:800[N</code></td>
      <td><code class="highlighter-rouge">1    500 .   N   &lt;TRA&gt;   ... CHR2=1;END=800;CT='5to5'</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1   500 .   N   N]1:800]</code></td>
      <td><code class="highlighter-rouge">1    500 .   N   &lt;TRA&gt;   ... CHR2=1;END=800;CT='3to3'</code></td>
    </tr>
  </tbody>
</table>

<p>Note again that the adjacencies can be described from either side.  To flip these calls is fairly straightforward; the ends of the intervals (5’ or 3’) belong to their respective breakpoint positions, so flipping the breakpoints just means flipping the positions, and the first and second number in the CT record.  So 3to3 and 5to5 remain the same, while 3to5 becomes 5to3 and vice versa.</p>

<p>Clearly, both <tra> calls and BND-style calls are equivalent; we output BND-style entries for conciseness, but either would work.</tra></p>

<h4 id="higher-level-symbolic-calls-del-inv-dup">Higher-level Symbolic Calls (&lt;DEL&gt;, &lt;INV&gt;, &lt;DUP&gt;)</h4>

<p>Higher-level calls have to be handled a little differently.  Because some
callers may call them using BND-style calls, or even &lt;TRA&gt; calls, they have to be decompose into the lowest common denominator - individual adjacencies.</p>

<p>In the figure below are simple examples of a deletion, an inversion, and a duplication.</p>

<p><img src="/assets/sv/del-inv-alpha.png" alt="VCF Breakpoint meanings" /></p>

<p>The deletion and duplication each generate only one novel adjacency (the 3to5 adjacency between Chr1:10 and Chr1:11 isn’t novel!), wheras the inversion generates 2.</p>

<p>Converting these into BND-style descriptions of the adjacencies follows below.</p>

<table>
  <thead>
    <tr>
      <th>Symbolic Call</th>
      <th>As BND call(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">1    10 .   N   &lt;DEL&gt;   ... END=20;</code></td>
      <td><code class="highlighter-rouge">1	10	.	N	N[1:21[</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1    10 .   N   &lt;INV&gt;   ... END=20;</code></td>
      <td><code class="highlighter-rouge">1	10	.	N	N]1:20]</code></td>
    </tr>
    <tr>
      <td> </td>
      <td><code class="highlighter-rouge">1	11	.	N	[1:21[N</code></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">1    1 .   N   &lt;DUP&gt;   ... END=10;</code></td>
      <td><code class="highlighter-rouge">1	1	.	N	]1:10]N</code></td>
    </tr>
  </tbody>
</table>

<p>Breaking up calls like inversions into multiple calls proved important for understanding the comparison of results; we found several cases where some callers called an inversion, whereas others only identified one-half of the event.</p>

<h3 id="complications">Complications</h3>

<p>While the above points cover most of the cases, the rather loose standards in this area cause a number of small bookkeeping headaches.</p>

<p>While callers often specify  CT info fields for symbolic calls, they typically aren’t necessary (and are sometimes inconsistent with the intent of the caller, so suggesting an inverted duplication where a simple duplication is meant, or turning a deletion into a strange translocation, so we strip them out rather than interpreting them.</p>

<p>Similarly, some callers make BND-like calls but use SVCLASS info fields to label the call as a higher-level symbolic call, sometimes then leaving out other information which must then be inferred from the SVCLASS; these then have to be interpreted partly as BND-style calls and partly as symbolic calls.</p>

<h3 id="implementation">Implementation</h3>

<p>We are consider structural variants to be equal for the purposes of merging if they represent a novel adjacency between two “equivalent” breakends.  We do not distinguish between calls that (for instance) have insertions called at the new adjacency.</p>

<p>As with the simple variants, these variants are stored as a python dictionary-of-dictionaries.  Each dictionary is a “locationdict”, taking as a key a (chromosome, position, direction/strand) pair, and querying a key will match any position within the windowsize of that location.  The first key is the (lexicographically first) breakpoint location and direction/strand, and the second is the that of the second.  The final value is a list of callers making the call, and their VCF records for comparison.</p>

<p>By including direction/strand information, we avoid the issue of merging two breakpoints with positions close but indicating opposite half-intervals.  So for instance, in the inversion example above, ]1:20] and [1:21[ are clearly very close together in their starting locations; but they must not be merged, as they are entirely non-overlapping regions.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Identifying common SV calls between two different callers with different conventions for outputting VCF records isn’t a deep algorithmic problem, but lack of consistent documentation makes it more challenging than we had originally anticipated.  We hope that this writeup helps others who are need to do something similar.</p>
</content>
   </entry>
  
 
  
   <entry>
     <title>On Random vs. Streaming I/O Performance; Or Seek(), and You Shall Find &ndash; Eventually.</title>
     <link href="http://simpsonlab.github.io/2015/05/19/io-performance/"/>
     <updated>2015-05-19T00:00:00-07:00</updated>
     <id>simpsonlab.github.io/2015/05/19/io-performance</id>
     <content type="html"><p>Last week, <a href="http://ivory.idyll.org/blog/">Titus Brown</a> asked a question on <a href="https://twitter.com/ctitusbrown/status/599220862311469056">twitter</a> and <a href="https://github.com/dib-lab/khmer/issues/1002">github</a> which spurred a lot of discussion – what was the best way to randomly read a subset of entries in a FASTQ file?  He and others quickly mentioned the two basic approaches – randomly seeking around in the file, and streaming the file through and selecting at random – and there were discussions of the merits of each in both forums.</p>

<p>This workload is a great case study for looking some of the ins and outs of I/O performance in general, and the tradeoffs between streaming and random file access in particular.  The results can be a little surprising, and the exact numbers will necessarily be file system dependant: but on hard drives (and even more so on most cluster file systems), seeks will perform surprisingly poorly compared to streaming reads (the “Reservoir” approach in the plot below):</p>

<p><img src="/assets/io/uncompress-seek-vs-stream.png" alt="Streaming Reads vs. Seeks" /></p>

<p>and here we’ll talk about why.</p>

<h2 id="anatomy-of-a-file-system">Anatomy of a File System</h2>

<p>Unlike so many other pieces of software we have to deal with daily, the file system stack just works, and so we normally don’t have to spend much time thinking about what happens under the hood; but some level of familiarity with the moving parts in there and how they interact helps us better understand when we can and can’t get good performance out of that machine.</p>

<h3 id="iops-vs-bandwidth">IOPS vs Bandwidth</h3>

<p>A hard drive is a physical machine with moving parts, and the file system software stack is built around this (even in situations where this might not make sense any more, like with SSDs - about which more later).  Several metal platters are spinning at speeds from 7,200 to 15,000 revolutions per minute (a dizzying 120-500 revolutions per <em>second</em>), and to start any particular read or write operation requires the appropriate sector of the disk being under the read heads; an event that won’t happen, on average, until 1 to 4 milliseconds from now.</p>

<p>Both the drive controller hardware and the operating system work hard to maximize the efficiency of this physical system, re-arranging pending reads and writes in the queue to ensure that requests are processed as quickly as possible; this allows one read request to “jump the queue” if the sector it needs to read from is just about to arrive, rather than having it wait in line, possibly for more than one disk rotation.  While this can greatly help the throughput of a large number of unrelated operations, it can’t do much to speed a single threaded program’s stream of reads or writes.</p>

<p>This means that, for physical media, there is a limit to the number of (say) read I/O Operations Per Second (IOPS) that can be performed; the bottleneck could be at the filesystem level, or the disk controller, but it is normally at the level of the individual hard drive, where the least can be done about it.  As a result, even for quite good, new, hard drives, a <a href="http://en.wikipedia.org/wiki/IOPS#Mechanical_hard_drives">typical performance</a> might be say 250 IOPS.</p>

<p>On the other hand, once the sector is under the read head, a lot of data can be pulled in at once.  New hard disks typically have <a href="http://en.wikipedia.org/wiki/Disk_sector">block sizes</a> of 4KB, and all of that data can be slurped in essentially instantly.  A good hard disk and controller can easily provide sequential read rates (or bandwidth) of over 100MB/s.</p>

<h3 id="prefetching-and-caching-or-why-is-bandwidth-so-good">Prefetching and Caching, or: Why is Bandwidth so good?</h3>

<p>You, astute reader, will have noticed that the numbers in that sentence above don’t even come close to working out.  250 IOP/s times 4KB is something like 1 MB/s, not 100MB/s.  Where does that extra factor of 100 come from?</p>

<p>Much as the operating system and disk controller both work to schedule reads and writes so that they are collectively completed as quickly as possible, the entire input/output stack on modern operating systems is built to make sure that it speeds up I/O whenever possible – and it is extremely successful at doing this, when the I/O behaves predictably.  Predictable, here, means that there is a large degree of <a href="http://en.wikipedia.org/wiki/Locality_of_reference">locality of reference</a> in the access; either temporal locality (if you’ve accessed a piece of data recently, you’re likely to access it again), or spatial (if you’ve accessed a piece of data, you’re likely to access a nearby piece soon).</p>

<p>Temporal locality is handled through caching; data that is read in is kept handy in case it’s needed again.  There may be block caches on the disk drive or disk controller itself; within the Linux kernel there is a unified cache which caches both low-level blocks and high level pages worth of data (the memory-mapped file interface ties directly into this).  Directory information is cached there, too; you may have noticed that in doing an <code class="highlighter-rouge">ls -l</code> in a directory with a lot of files, the first is much slower than any followups.</p>

<p>In user space, I/O libraries often do their own caching, as well.  C’s stdlib, for instance, will cache substantial amounts of recently used data; and so, by extension, will everything built upon it (iostreams in C++, or lines of data seen by Python).  The various players are shown below in this diagram from IBM’s DeveloperWorks:</p>

<figure>
    <img src="http://www.ibm.com/developerworks/library/l-virtual-filesystem-switch/figure8.gif" alt="File System Stack, from IBM DeveloperWorks: http://www.ibm.com/developerworks/library/l-linux-filesystem-switch/" />
    <figcaption style="font-style:italic; font-size:75%">File System Stack, from IBM DeveloperWorks: http://www.ibm.com/developerworks/library/l-linux-filesystem-switch/</figcaption>
</figure>

<p>None of this caching directly helps us in our immediate problem, since we’re not intending to re-read a sequence again and again; we are picking a number of random entries to read.  However, the entire mechanism used for caching recently used data can also be used for presenting data that the operating system and libraries thinks is <em>going</em> to be used <em>soon</em>.  This is where the second locality comes in; spatial locality.</p>

<p>The Operating System and libraries make the reasonable assumption that if you are reading one block in a file, there’s an excellent chance that you’ll be reading the next block shortly afterwards. Since this is such a common scenario, and in fact one of the few workloads that can easily be predicted, the file system (at all levels) supports quite agressive  prefetching, or <a href="http://en.wikipedia.org/wiki/Readahead">read ahead</a>.  This basic idea – since reading is slow, try to read the next few things ahead of time, too – is so widely useful that it is used not just for data on disk, but for data in <a href="http://en.wikipedia.org/wiki/Prefetch_buffer">RAM</a>, links by <a href="https://medium.com/@luisvieira_gmr/html5-prefetch-1e54f6dda15d">web browsers</a>, etc.</p>

<p>To support this, the lowest levels of the file system (block device drivers, and even the disk and controller hardware) try to lay out sequential data on disk in such a way that when one block is read, the next block is immediately ready to be read, so that only one seek, one IOP, is necessary to begin the read, and then following reads happen more or less “for free”.  The higher levels of the stack take advantage of this by explicitly requesting one or many pages worth of data whenever a read occurs, and presents that data in the cache as if it had already been used.  Then this data can be accessed by user software without expensive I/O operations.</p>

<p>The effectiveness of this can be seen not only in the factor of 100 difference in streaming reads (100MB/s vs 4KB x 250 IOP/s), but also in how performance suffers when this isn’t possible.  On a hard drive that is nearly full, a new file being written doesn’t have the luxury of being written out in nice contiguous chunks that can be read in as a stream.  The disk is said to be <em>fragmented</em>, and <a href="http://en.wikipedia.org/wiki/Defragmentation">defragmentation</a> can often improve performance.</p>

<p>In summary, then, prefetching and caching performed by the disk, controller, operating system, and libraries can speed large streaming reads on hard disks by a factor of 100 over random seek-and-read patterns, to the extent that, on a typical hard drive, 100-400KB can be read in the time that it takes to perform a single seek.  On these same hard drives, then, you might expect streaming through a single 1000MB file to take roughly as long (~10s) as 2,500-4,000 seeks.  We’ll see later that considering other types of file systems - single SSDs, or clustered file systems - can change where that crossover point between number of seeks versus size of streaming read will occur, but the basic tradeoff remains.</p>

<h2 id="the-random-fasta-entry-problem">The Random FASTA Entry Problem</h2>

<p>To illustrate the performance of both a seeking and sequential streaming method, let’s consider a slightly simpler problem than posed.  To avoid the complications with FASTQ, let’s consider a sizeable FASTA file (we take <a href="http://hgdownload.cse.ucsc.edu/goldenpath/hg19/bigZips/est.fa.gz">est.fa from HG19</a>, slightly truncated for the purposes of some of our later tests).  The final file is about 8,000,000 lines of text, containing some 6,444,875 records.  We consider both compressed and uncompressed versions of the file.</p>

<p>We’ll randomly sample <script type="math/tex">k</script> records from this file – 0.1%, 0.2%, 0.5%, 1%, 2%, 5%, 10%, 20%, and 50% of the total number of records <script type="math/tex">N</script> – run for several different trials, and done a few different ways.  We’ll consider a seeking solution and a sequential reading solution.</p>

<p>The code we’ll discuss below, and scripts to reproduce the results, can be found <a href="https://github.com/ljdursi/seek-vs-sequential">on GitHub</a>.  The code is in Python, for clarity.</p>

<h3 id="a-seeking-solution">A Seeking Solution</h3>

<p>Coding up a solution to randomly sample the file by seeking to random locations is relatively straightforward.  We generate a random set of offsets into the file, given the file’s size; then seek to each of these locations in order, find and read the next record, and continue.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">randomSeek</span><span class="p">(</span><span class="n">infile</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">nrecords</span><span class="p">):</span>
    <span class="n">infile</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">SEEK_SET</span><span class="p">)</span>

    <span class="n">totrecords</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">recordsdict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">while</span> <span class="n">totrecords</span> <span class="o">&lt;</span> <span class="n">nrecords</span><span class="p">:</span>
        <span class="c"># generate the random locations</span>
        <span class="n">locations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nrecords</span><span class="o">-</span><span class="n">totrecords</span><span class="p">):</span>
            <span class="n">locations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">size</span><span class="p">)))</span>
        <span class="n">locations</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

	<span class="c"># read the records immediately following these locations</span>
        <span class="n">reader</span> <span class="o">=</span> <span class="n">simplefasta</span><span class="o">.</span><span class="n">FastaReader</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">location</span> <span class="ow">in</span> <span class="n">locations</span><span class="p">:</span>
            <span class="n">infile</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="n">location</span><span class="p">,</span><span class="n">os</span><span class="o">.</span><span class="n">SEEK_SET</span><span class="p">)</span>
            <span class="n">reader</span><span class="o">.</span><span class="n">skipAhead</span><span class="p">()</span>
            <span class="n">record</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">readNext</span><span class="p">()</span>
            <span class="n">recordsdict</span><span class="p">[</span><span class="n">record</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">totrecords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">recordsdict</span><span class="p">)</span>
        
    <span class="c"># return a list of records</span>
    <span class="n">records</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">recordsdict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">records</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">records</span></code></pre></figure>

<p>There are a few things to note about this approach:</p>

<ul>
  <li>I’ve sorted the random locations to improve our chances of reading nearby locations sequentially, letting the operating system help us when it can.</li>
  <li>The file size must be known before generating the locations.  This means we must have access to the whole file; we can’t just pipe a file through this method</li>
  <li>Some compression methods become difficult to deal with; one can’t just move to a random location in a gzipped file, for instance, and start reading.  Other compression methods - bgzip, for instance, make things a little easier but still tricky.</li>
  <li>The method is not completely uniformly random if the records are of unequal length; we are more likely to land in the middle of a large record than a small one, so this method is biased in favour of records following a large one.</li>
  <li>Because we are randomly selecting locations, we may end up choosing the same record more than once; this gets more likely as the fraction of records we are reading increases.  In this case, we go back and randomly sample records to make up the difference.  There’s no way to know in general if two locations indicate the same record without reading the file at those locations.</li>
</ul>

<h3 id="a-streaming-solution---reservoir-sampling">A Streaming Solution - Reservoir Sampling</h3>

<p>A solution to the <script type="math/tex">k</script>-random-sampling problem which makes use of streaming input is the <a href="http://en.wikipedia.org/wiki/Reservoir_sampling">Reservoir Sampling</a> approach.  This elegant method not only takes advantage of streaming support in the file system, but doesn’t require knowledge of the file size ahead of time; it is a so-called <a href="http://en.wikipedia.org/wiki/Online_algorithm">‘online’ method</a>.</p>

<p>The basic idea is that, for every <script type="math/tex">i</script>th item seen, it is selected with a probability of <script type="math/tex">k/i</script>.  Because there’s a <script type="math/tex">1/(i+1)</script> chance of it being bumped by the next item, then the probability of the <script type="math/tex">i</script>th item being selected by the end of round <script type="math/tex">i+1</script> is</p>

<script type="math/tex; mode=display">P_{i+1}(i) = P_i(i) \times \left (1 - \frac{1}{i+1} \right) = \frac{k}{i} \frac{i}{i+1} = \frac{k}{i+1}</script>

<p>and so on until by the time all <script type="math/tex">N</script> items are read, each item has a <script type="math/tex">k/N</script> chance of being selected.</p>

<p>Our simple implementation follows; we select the first <script type="math/tex">k</script> items to fill the reservoir, and then randomly select through the rest of the file.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">reservoir</span><span class="p">(</span><span class="n">infile</span><span class="p">,</span> <span class="n">nrecords</span><span class="p">):</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">simplefasta</span><span class="o">.</span><span class="n">FastaReader</span><span class="p">(</span><span class="n">infile</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">nrecords</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nrecords</span><span class="p">):</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">readNext</span><span class="p">()</span>

    <span class="n">countsSoFar</span> <span class="o">=</span> <span class="n">nrecords</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">reader</span><span class="o">.</span><span class="n">eof</span><span class="p">():</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">countsSoFar</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loc</span> <span class="o">&lt;</span> <span class="n">nrecords</span><span class="p">:</span>
            <span class="n">record</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">readNext</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">record</span><span class="p">:</span>
                <span class="n">results</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">record</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reader</span><span class="o">.</span><span class="n">skipAhead</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">countsSoFar</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">results</span></code></pre></figure>

<p>Note that this method does uniformly select records, and can work in pipes or through compressed files equally well as it passes sequentially through the entire file.</p>

<h3 id="timing-results-workstation-hard-drive">Timing Results: Workstation Hard Drive</h3>

<p>I ran these benchmarks on my desktop workstation, with a mechanical hard drive. As a quick benchmark for streaming, simply counting the number of lines of the uncompressed (a shade under 4GB) file takes about 25 seconds on this system, which gives us a sense of the best possible streaming time for the file; this is about 160MB/s, a reasonable result (and in fact slightly higher than I would have expected).  Similarly, if we expected an IOPS rate of about 400, then we’d expect to see 0.1% selection to take about 6445/400 ~ 16s.</p>

<p>The file handling and parsing will be significantly slower in python than it would be in C++, which disadvantages the streaming approach (which must process many more records than the seeking approach) somewhat, but our results should be instructive regardless.</p>

<p>The primary results are shown in this plot, which we have already seen (note that both x and y scales are logarithmic):</p>

<p><img src="/assets/io/uncompress-seek-vs-stream.png" alt="Streaming Reads vs. Seeks" /></p>

<p>Some basic takeaways:</p>

<ul>
  <li>The reservoir approach, which always has to pass through the entire file, is much less variable in time than the seeking approach.</li>
  <li>For sampling less than 0.75% of the file, seeking is clearly and reliably faster; for greater sampling fraction, seeking may or may not be faster</li>
  <li>At very large fractions, the seeking time blows up as the chance of “collisions” - selecting the same entry multiple times - greatly increases, meaning you have to go back and resample.  But no one would really suggest this approach for sampling more than 10% of the file anyway.</li>
  <li>Reservoir sampling works roughly equally well if it is operating directly on the file or having it piped through; and it actually can be somewhat faster for a gzipped file with zcat, since less data actually has to be pulled from the disk.</li>
</ul>

<p>We also tried the same test on gzipped files directly, since Python’s <a href="https://docs.python.org/2/library/gzip.html">gzipped file access</a> has a seek operation you can in principle use; but this isn’t really a fair test, as you can’t properly <code class="highlighter-rouge">seek</code> through a gzipped file, you have to decompress along the way.  That means the “seeking” approach is really just a streaming approach implemented much less efficiently, and we see that quite clearly:</p>

<p><img src="/assets/io/gzip-seek-vs-stream.png" alt="On gzipped files" /></p>

<p>It was because the file was truncated that we could use gzip with seek-based sampling here at all; seek-sampling requires knowing the filesize, and the total (uncompressed) file size isn’t available with a gzipped file unless the uncompressed size is less than 4GB.</p>

<h3 id="sidebar---benchmark-warning-clear-that-cache">Sidebar - Benchmark warning: Clear that cache!</h3>

<p>Note that for benchmarking I/O, even for moderately large files like this one (~1+GB compressed, 4GB uncompressed), a significant amount of the file will remain in various levels of OS cache, so it is absolutely essential to clear or avoid the cache in subsequent runs or else your timing results will be completely wrong.</p>

<p>On linux, as root, you can completely clear caches with the following:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>sync
<span class="gp">$ </span><span class="nb">echo </span>3 &gt; /proc/sys/vm/drop_caches</code></pre></figure>

<p>but this is rather overkill, and requires root.  Easier, but requiring more disk space (and a file system which is not too smart/aggressive about deduplication!) is to cycle between multiple copies of the same file.  The <code class="highlighter-rouge">getdata</code> script makes 5 copies each of the compressed and uncompressed est.trunc.fa file to cycle through, which may or may not be enough.</p>

<h2 id="other-file-stores-cluster-file-systems">Other File Stores: Cluster File Systems</h2>

<p>Of course, single spinning disk performance isn’t all that a bioinformatician cares about.  Two other types of file systems play a large role; file systems on shared clusters, and individual SSDs.</p>

<p>On a cluster file system, data is stored on a large array of spinning disks.  This has performance advantages, and disadvantages.</p>

<p>On the plus side, file systems like <a href="http://en.wikipedia.org/wiki/Lustre_(file_system)">Lustre</a> will often stripe large files across several disks and even servers, so that streaming reads can be served from many pieces of hardware at once – potentially offering many multiples of 100MB/s of streaming bandwidth.   Similarly, the data file being striped across multiple disks means that many multiples of the IOPS are in principle available.</p>

<p>In practice, because so many parts (servers, disks) need to be coordinated to perform operations, there is often an additional latency to file system operations; this tends to come through as modestly fewer effective IOPS than one would expect.  This means that the IOPS, while still potentially much higher than a local disk, are proportionally less increased than the bandwidth, tilting the balance in favour of streaming over seeking.</p>

<p>On the other hand, having a network-attached file system introduces another potential bottleneck; a slow or congested network may mean that the peak bandwidth available for streaming reads at the reader may be decreased, pushing the balance back towards seeking.</p>

<h2 id="other-file-stores-ssds">Other File Stores: SSDs</h2>

<p>SSDs – which are ubiquitous in laptops and increasingly common on workstations – change things quite a bit.  These solid state devices have no moving parts, meaning that there is no delay waiting for media to move to the right location.  As a result, IOPS on these devices can be <a href="http://en.wikipedia.org/wiki/IOPS#Solid-state_devices">significantly higher</a>. Indeed, traditional disk controllers and drivers become the bottleneck; a consumer-grade device plugged in as a disk will still be limited to 500MB/s and say 20k IOPS, while specialized devices that look more directly like external memory can achieve much higher speeds.  (For those who want to know more about SSDs, Lee Hutchinson has an <a href="http://arstechnica.com/information-technology/2012/06/inside-the-ssd-revolution-how-solid-state-disks-really-work/">epic and accessible discussion of how SSDs work</a> on Ars Technica; the article is from 2012 but very little fundamental has changed in the intervening three years).</p>

<p>At those rates, both streaming and seeking workflows see a performance boost, but the increase is much higher for IOPS.  Rather than streaming a 1000MB file taking roughly as long as 2,500-4,000 seeks, it is now more like 40,000 seeks.  That’s still finite, and each seek still takes roughly as much time as reading 25KB of data; but that factor of ten difference in relative rates will change the balance between whether streaming or seeking is most efficient for any given problem.</p>

<p>Running this same test on my laptop gives results as shown below:</p>

<p><img src="/assets/io/ssd.png" alt="SSD: Streaming Reads vs. Seeks" /></p>

<p>We see that the laptop-grade hardware limits the performance of the streaming read; bandwidths (and thus the performance of the reservoir sampling) are down by about a factor of 2.  On the other hand, we seem to have gained over a factor of 10 in IOPS, with approximately 3000 effective random reading IOPS.  As a result, the seeking for a 0.1% sampling fraction takes a lightning-fast 2.5 seconds.</p>

<p>However, it’s worth noticing that, even with the decrease in bandwidth and startling increase in IOPS, the crossover point between where streaming wins over seeking has only shifted from 0.75% to 3%; beyond that, streaming is clearly the winner.</p>

<h2 id="how-to-further-improve-seeking-results">How to further improve seeking results</h2>

<h3 id="hardware-ssds">Hardware: SSDs</h3>

<p>Mechanical hard drives will always be at a significant disadvantage for random-access workloads compared to SSDs.  While SSDs are significantly more expensive than mechanical HDs for the same capacity, the increase in performance for these workloads (and their lower power draw for laptops) may make them a worthwhile investment for use cases where seeky access can’t be avoided.</p>

<h3 id="software-multithreading">Software: multithreading</h3>

<p>It’s also possible to improve the performance of the seeky workload through software.  As mentioned before, the file system OS layer and physical layer are highly concurrent, juggling many requests at once and shuffling the order of requests behind the scenes to maximize throughput.  For a highly seeky workflow like this, it’s often possible to make use of this concurrency by launching multiple threads, each sending their read request at the same time, and waiting until completion before launching the next.  This greatly increases the chance of finding a read request which can be performed quickly, making fuller use of the disk subsystem.  This significantly increases the complexity of the user software, however, and I won’t attempt it for the purposes of this post.</p>

<h3 id="software-turning-off-os-caching">Software: turning off OS caching</h3>

<p>A smaller possible gain could be realized, for small sample fractions, by hinting to the operating system not to provide expensive caching that will not be used by the seek-heavy access pattern.  This can be done by <a href="http://man7.org/linux/man-pages/man2/open.2.html">opening the file with O_DIRECT</a>, or using <a href="https://docs.python.org/dev/library/os.html#os.posix_fadvise">posix_fadvise</a> which allows a more flexible method for hinting to the operating system not to bother prefetching or caching, respectively, by passing <code class="highlighter-rouge">POSIX_FADV_RANDOM </code>and <code class="highlighter-rouge">POSIX_FADV_NOREUSE</code>.  However, this is likely only helpful for very small sample fractions, where seeking is already doing pretty well; for moderate sample fractions, the prefetching can actually help (e.g., that downward trend in time taken at around 10%) so I did not include this in the benchmark.</p>

<h2 id="how-to-further-improve-sequential-results">How to further improve sequential results</h2>

<h3 id="hardware-ssds-1">Hardware: SSDs</h3>

<p>Workstation-class SSDs, with appropriate controllers, also offer a significant
increase in streaming bandwidth over their mechanical counterparts, even if the
increase is proportionally less than that in IOPS.  4-5x increases are not
uncommon, and those would benefit the reservoir method here.</p>

<h3 id="software-faster-parsing">Software: Faster parsing</h3>

<p>While Python is excellent for many purposes, there is no question but that it is
slower than compiled languages like C++.  FASTA parsing is quite simple, and
for very small sampling fractions, there is no good reason that the resevoir solution should be a
factor of two or more slower than running <code class="highlighter-rouge">wc -l</code>.  This hurts the reservoir
sampling more than the streaming, as the resevoir approach (which parses records which then
get bumped later) must parse ~17 times more records in this test than the seeking method.</p>

<h3 id="software-turning-off-os-caching-1">Software: turning off OS caching</h3>

<p>While <em>prefetching</em> is essential for the streaming performance we have seen,
there may be some modest benefit to turning off <em>caching</em> of the data we 
read in; after all, even with the reservoir sampling, we are still only
processing each record at most once.  Again, we could use
<a href="https://docs.python.org/dev/library/os.html#os.posix_fadvise">posix_fadvise</a>,
this time with only <code class="highlighter-rouge">POSIX_FADV_NOREUSE</code>.  I again expect this to be a
relatively small effect, and so it is not tested here.</p>

<h2 id="conclusion-io-is-complicated-but-streaming-is-pretty-fast">Conclusion: I/O is Complicated, But Streaming is Pretty Fast</h2>

<p>This post only scratches the surface of I/O performance considerations.  While we’ve thought a little bit about seeking vs sequential reading IOPS and bandwidth, even just streaming writes has different considerations (on the one hand, you can write anywhere that’s free, as opposed to needing to read specific bytes; on the other hand, there’s nothing like ‘prefetching’ for reads).  More complex operations – and especially metadata-heavy operations, like creating, deleting, or even just appending to files – involve even more moving parts.</p>

<p>And even for seeking vs streaming reads, while the trends we’ve discussed here are widely applicable, different systems – underlying hardware (disk vs ssd) or how the file system is accessed (network-attached vs local) can greatly change the underlying numbers, which change the tradeoffs between seeking and streaming, possibly enough to make the conclusions different for any particular use case.  We are empiricists, and the best way to find out which approach works best for your problem is to measure on the system that you use – fully aware that anyone else who uses your software might do so on different systems or for different-sized problems.</p>

<p>But a lot of software and hardware infrastructure is tuned to make streaming reads from filesystems as fast as possible, and it’s always worth testing to see if streaming through the data really isn’t fast enough.</p>
</content>
   </entry>
  
 
  
   <entry>
     <title>Understanding Partial Order Alignment for Multiple Sequence Alignment</title>
     <link href="http://simpsonlab.github.io/2015/05/01/understanding-poa/"/>
     <updated>2015-05-01T00:00:00-07:00</updated>
     <id>simpsonlab.github.io/2015/05/01/understanding-poa</id>
     <content type="html"><p>Jared’s <a href="https://github.com/jts/nanopolish">nanopolish</a> tool for
Nanopore data uses <a href="http://sourceforge.net/projects/poamsa/">poaV2</a>,
the original partial order alignment software described in papers by
Lee, Grasso, and Sharlow <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup><sup>,</sup><sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup><sup>,</sup><sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>, for
correcting the reads, following a similar approach taken by PacBio in
<a href="https://github.com/PacificBiosciences/pbdagcon">PBDagCon</a>.</p>

<p>This post gives a quick lower-level overview of the steps in the POA
algorithm, with a <a href="https://github.com/ljdursi/poapy">simple implementation in python</a> 
to demonstrate the ideas more concretely.</p>

<h3 id="the-basics">The Basics</h3>

<p>The insight of the first POA paper was that “flattening” of
the alignment of sequences leads to meaningless artifacts that, while
largely harmless for pairwise alignments or even multiple alignments
of strongly conserved sequences, causes problems with more general
multiple alignments.  For instance, consider the following sequences:</p>

<pre>
&gt;seq1
CCGCTTTTCCGC
&gt;seq2
CCGCAAAACCGC
</pre>

<p>There is ambiguity in selecting a single, best alignment between
this pair of sequences; for instance below are 4 of</p>
<s>2<sup>8</sup>&nbsp;=&nbsp;256</s>
<p>8 choose 4 = 105 nearly equivalent
ways of expressing this pairwise alignment. The best alignment will
depend on the particular gap-scoring scheme used.</p>

<pre>
CCGC----TTTTCGCG   CCGCTTTT----CCGC  CCGC-TT-TT--CGCG   CCGC-T-T-T-TCCGC
CCGCAAAA----CGCG   CCGC----AAAACCGC  CCGCA--A--AACCGC   CCGCA-A-A-A-CCGC
</pre>

<p>While for a pairwise alignment this is comparatively harmless, as
additional sequences are added to form a multiple sequence alignment
(MSA), the choice between these ambiguities begin to distort the eventual
result. What we would like is to consider not necessarily a single linear
layout, but something that can express more unambiguously “one
sequence inserts a run of A, and the other of T”. And a natural way
to view that is with a graph:</p>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/3.11.0/vis.min.js"></script>

<div id="mynetwork"></div>

<script type="text/javascript">
var nodes = [
    {id:0, label: "C", allowedToMoveX: false, x: 0, y: 0 , allowedToMoveY: true }, {id:1, label: "C", allowedToMoveX: false, x: 150, y: 0 , allowedToMoveY: true },
    {id:2, label: "G", allowedToMoveX: false, x: 300, y: 0 , allowedToMoveY: true }, {id:3, label: "C", allowedToMoveX: false, x: 450, y: 0 , allowedToMoveY: true },
    {id:12, label: "A", allowedToMoveX: false, x: 600, y: 0 , allowedToMoveY: true }, {id:4, label: "T"},
    {id:13, label: "A", allowedToMoveX: false, x: 750, y: 0 , allowedToMoveY: true }, {id:5, label: "T"},
    {id:14, label: "A", allowedToMoveX: false, x: 900, y: 0 , allowedToMoveY: true }, {id:6, label: "T"},
    {id:15, label: "A", allowedToMoveX: false, x: 1050, y: 0 , allowedToMoveY: true }, {id:7, label: "T"},
    {id:8, label: "C", allowedToMoveX: false, x: 1200, y: 0 , allowedToMoveY: true }, {id:9, label: "C", allowedToMoveX: false, x: 1350, y: 0 , allowedToMoveY: true },
    {id:10, label: "G", allowedToMoveX: false, x: 1500, y: 0 , allowedToMoveY: true }, {id:11, label: "C", allowedToMoveX: false, x: 1650, y: 0 , allowedToMoveY: true }
];
 
var edges = [
    {from: 0, to: 1, value: 3}, {from: 1, to: 2, value: 3}, {from: 2, to: 3, value: 3}, {from: 3, to: 12, value: 2}, {from: 3, to: 4, value: 2},
    {from: 12, to: 13, value: 2}, {from: 4, to: 5, value: 2}, {from: 13, to: 14, value: 2}, {from: 5, to: 6, value: 2}, {from: 14, to: 15, value: 2},
    {from: 6, to: 7, value: 2}, {from: 15, to: 8, value: 2}, {from: 7, to: 8, value: 2}, {from: 8, to: 9, value: 3}, {from: 9, to: 10, value: 3},
    {from: 10, to: 11, value: 3}
];

  var container = document.getElementById('mynetwork');
  var data= { nodes: nodes, edges: edges, };
  var options = { width: '100%', height: '200px' };
  var network = new vis.Network(container, data, options);
</script>

<p>The partial order alignment graph differs from the alignment strings
in that a given base can have multiple predecessors (<em>eg</em>, the <code class="highlighter-rouge">C</code>
after the fork being preceeded by both a string of <code class="highlighter-rouge">A</code>s and of <code class="highlighter-rouge">T</code>s)
or successors (<em>eg</em>, the <code class="highlighter-rouge">C</code> before the fork).  But it is similar to
the alignment strings in that there is a directional order imposed,
both in the sense that each node has (zero or more) predecessors and
(zero or more) successors, but also that no repetition, or doubling
back, is allowed; the graph is constrained to be a <a href="http://en.wikipedia.org/wiki/Directed_acyclic_graph">Directed, Acyclic
Graph</a> (DAG).</p>

<p>Both repeats and re-orderings can be biologically relevant, and various
generalizations of alignment have allowed this <sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup><sup>,</sup><sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>.
This greatly generalizes the problem, moving it closer to assembly.
For the purposes of error correction in nanopolish, that additional
generalization is not needed.</p>

<h3 id="smith-waterman">Smith-Waterman</h3>

<p>To consider how alignment to a graph works, let ’s remind ourselves of
how we perform alignment on sequences.</p>

<p>In the
<a href="http://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm">Needleman-Wunsch</a>
algorithm and its variants, we consider two cursors - one on a base in
each sequence.  For each pair of cursor positions in turn, we consider
the question of “what is the best sequence of alignments and insertions
that could lead to this position in the alignment”.  Because the globally
optimal path must be made from locally optimal “moves” (that is, the
“principle of optimality” holds for this problem), this reduces to finding
out which of the three possible moves that would advance the cursors to
this position to choose from:</p>

<ul>
  <li>Both cursors having advanced, aligning (matching) these two bases;</li>
  <li>Cursor 1 had advanced while cursor 2 remained fixed, inserting that base from sequence 1 into the alignment</li>
  <li>Vice versa, with cursor 2 advancing and cursor 1 staying fixed.</li>
</ul>

<p>A familiar diagram follows below; of those three possible moves, we take the
running scores from each of those previous positions, add the score
corresponding to the move, and set the score of the current position.</p>

<p><img src="/assets/poa/sw-dynamicprogramming.png" alt="Dynamic programming for string-string alignment" title="Dynamic programming for string-string alignment" /></p>

<p>We can calculate the scores for pairs of positions in any order we like
– along rows of the matrix, columns, or minor diagonals –
as long as for any position we calculate, the scores for the previous 
positions we need have already been calculated.</p>

<h3 id="string-to-graph-alignment">String to Graph Alignment</h3>

<p><img src="/assets/poa/poa-dynamicprogramming.png" alt="Dynamic programming for graph-string alignment" title="Dynamic programming for graph-string alignment" /></p>

<p>Aligning a sequence to a DAG introduces suprisingly little complexity to the
dynamic programming problem; the clever diagram in the POA paper with a dynamic
programming matrix with 3D “bumps” may have had the unintended consequence of
making it look more complicated than it is.</p>

<p>The primary difference for the purposes of dynamic programming is
that while a base in a sequence has exactly one predecessor, a base
in a graph can have two or more.  Thus, the cursor may have come from
one of several previous locations for the same (graph) Insert or Align
moves being considered; and thus those scores must be considered too in
determining the best previous position.  (Note that insertions from the
sequence are unchanged).</p>

<p>So, to reiterate: the only difference deep inside the dynamic programming
loop is that multiple previous scores (and any associated gap-open
information) must be considered for insertions or alignments of the
graph base.  This is implemented by a loop over predecessors for the current
base, and all else remains the same.</p>

<h3 id="topological-sort">Topological Sort</h3>

<p>There is one step that has to happen <em>before</em> that dynamic programming loop,
however.</p>

<p>When aligning two sequences, one could choose an order to loop over the
sequence indices before hand so that, for any new position being calculated,
the necessary previous scores would already be ready.</p>

<p>The nodes in the graph, however, do not have such a useful intrinsic
order.  If the nodes are considered in the order they are added, for
instance, then the newest nodes inserted with a new sequence –
which may have been inserted as predecessors of nodes that had been
inserted earlier – will not have been already scored when their
successor begins its calculation.</p>

<p>The answer is to use a <a href="http://en.wikipedia.org/wiki/Topological_sorting">Topological Sort</a> to generate an
ordering of nodes in which every node is guaranteed to follow all of
its predecessors.  This is always possible for a directed graph as long
as there are no cycles, and indeed there can be many such orderings.
Topological sorts are how <code class="highlighter-rouge">make</code> and similar tools decide in which order 
to perform tasks in a workflow, and how many<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup> spreadsheet programs decide 
if cells need to be updated.</p>

<p>There are two main classes of algorithms for performing topological sorts; the
algorithm of Kahn (1962), and a repeated depth-first search.  Either serves
perfectly well for the dynamic programming problem.</p>

<p>So to align a sequence to a graph, the steps are simply:</p>

<ul>
  <li>Perform a topological sort if the graph has been updated</li>
  <li>Do the dynamic programming step as usual, with:
    <ul>
      <li>The graph nodes visited in the order of the topological sort, and</li>
      <li>Considering all valid predecessors for align/insert moves.</li>
    </ul>
  </li>
</ul>

<h3 id="insertion-of-aligned-sequence">Insertion of aligned sequence</h3>

<p>Consider that we have a graph that so far only contains the sequence <code class="highlighter-rouge">CGCTTAT</code>,
and our dynamic programming calculation aligning the sequence <code class="highlighter-rouge">CGATTACG</code> has
given us an alignment that looks like this:</p>

<pre>
CGATTACG
||.|||.
CGCTTAT-
</pre>

<p>That is, for each base in the sequence, it is paired (either as match or
mismatch) with a base in the graph, or it is inserted.</p>

<p>We expect inserting the new sequence into the graph to give us something like:</p>

<div id="mynetwork2"></div>
<script type="text/javascript">
  // create a network
var nodes = [
    {id:0, label: "C", allowedToMoveX: false, x: 0, y: 0 , allowedToMoveY: true }, {id:1, label: "G", allowedToMoveX: false, x: 150, y: 0 , allowedToMoveY: true },
    {id:8, label: "C", allowedToMoveX: false, x: 300, y: 0 , allowedToMoveY: true }, {id:2, label: "A"},
    {id:3, label: "T", allowedToMoveX: false, x: 450, y: 0 , allowedToMoveY: true }, {id:4, label: "T", allowedToMoveX: false, x: 600, y: 0 , allowedToMoveY: true },
    {id:5, label: "A", allowedToMoveX: false, x: 750, y: 0 , allowedToMoveY: true }, {id:9, label: "T"},
    {id:6, label: "C", allowedToMoveX: false, x: 900, y: 0 , allowedToMoveY: true }, {id:7, label: "G", allowedToMoveX: false, x: 1050, y: 0 , allowedToMoveY: true }
];
 
var edges = [
    {from: 0, to: 1, value: 3}, {from: 1, to: 8, value: 2}, {from: 1, to: 2, value: 2},
    {from: 8, to: 3, value: 2}, {from: 2, to: 3, value: 2}, {from: 2, to: 8, value: 1, style: "dash-line"},
    {from: 3, to: 4, value: 3}, {from: 4, to: 5, value: 3}, {from: 5, to: 9, value: 2},
    {from: 5, to: 6, value: 2}, {from: 6, to: 7, value: 2}, {from: 6, to: 9, value: 1, style: "dash-line"}
];

  var container = document.getElementById('mynetwork2');
  var data= { nodes: nodes, edges: edges, };
  var options = { width: '100%', height: '200px' };
  var network = new vis.Network(container, data, options);
</script>

<p>Here we see for the first time two types of edges; bold, directed edges
(with directions not shown, but left-to-right), indicating
predecessor/successor; and dashed lines, indicating that (say) the <code class="highlighter-rouge">A</code> and <code class="highlighter-rouge">C</code>
that are three bases from the start are aligned to each other, but are
mismatches; similarly with the <code class="highlighter-rouge">C</code> and <code class="highlighter-rouge">T</code> towards the end.</p>

<p>We keep track of both the predecessor/successor nodes and all ‘aligned-to’
nodes.  We walk along the sequence we are inserting and its calculated
alignment.  We insert nodes in the sequence if they are not aligned to
anything, or none of the nodes that it directly or indirectly aligns to have
the same base; otherwise, we re-use that node and simply add new edges to it if
necessary.</p>

<p>In more detail, the steps we take are as follows:</p>

<ul>
  <li>A new “starting point” for this sequence is created in the graph.</li>
  <li>The previous position is set to this starting point.</li>
  <li>For each sequence base in the calculated alignment,
    <ul>
      <li>If the current base is not aligned to a node in the graph, or if it is but neither the node nor any node <em>it</em> is aligned to has the same base,
        <ul>
          <li>A new node is created with the sequence base, and is selected as the current node</li>
          <li>This new node is aligned to the aligned node if any, and all of the “aligned-to” nodes are updated to align to this one.</li>
        </ul>
      </li>
      <li>Otherwise,
        <ul>
          <li>That node with the same base is selected as the current node</li>
        </ul>
      </li>
      <li>If one does not already exist, a new edge is added from the previous position to the current node</li>
      <li>That edge has the current sequence label added to it; the number of labels on the edge correspond to the number of sequences that include that edge and those two nodes.</li>
    </ul>
  </li>
</ul>

<p>Fusing nodes whenever possible ensures that information about a motif that
several times in several sequences in a similar location is not obscured
by corresponding to several paths through the graph; It also increases
the runtime of the algorithm by limiting the number of nodes and edges that
need to be considered.</p>

<p>Note that one can always reconstruct any individual sequence inserted into the
graph by looking up its starting point, and following edges labelled with the
corresponding label through the graph.</p>

<p>Once an aligned sequence is inserted, a new topological sort of the nodes is
generated, and another alignment can be perfomed.</p>

<h3 id="consensus-paths">Consensus paths</h3>

<p>Now that you have all of your sequences in the graph, how do you get things
like a consensus sequence out of it?  This is the topic of a paper<sup id="fnref:2:1"><a href="#fn:2" class="footnote">2</a></sup> separate
from the first one.</p>

<p>Finding the single best-supported traversal through the graph is relatively
straightforward.  In fact, this is again a dynamic programming problem; one
sets the scores of all nodes to zero, and then marches through the graph node
by node.  At each node, one chooses the “best” edge into that node – the
one with the most sequences including it – and sets the score to be 
the edge weight plus the score of the node pointed to; and in case of a tie
between edges, one chooses the one pointing to the highest-scoring node.</p>

<p>The highest score and the edges chosen gives you a maximum-weighted path
through the graph.  As is pointed out in the consensus paper, this is
a maximum-likelihood path if the edge weights correspond to the probabilities
that the edge is followed.</p>

<p>However, there may well be multiple consensus features in the alignment that
one wishes to extract; a feature seen by multiple but still a minority of
sequences.  The approach to finding remaining consenses is necessarily somewhat
heuristic, and comprises the bulk of the consensus paper.</p>

<p>The basic idea is to somehow remove or downweight the edges that
correspond to the already-extracted consenses, and repeat the procedure
to find additional features.  The steps recommended in the consensus paper are:</p>

<ul>
  <li>Identify sequences that correspond to the consensus just identified; by (<em>eg</em>) fraction of their bases/edges included, possibly with other requirements</li>
  <li>For edges corresponding to those sequences, reduce the weight corresponding to those sequences, possibly to zero</li>
  <li>Rerun the consensus algorithm.</li>
</ul>

<p>In the simple implementation we use to demonstrate these ideas, we simply
choose all (remaining) sequences that have a majority of their bases
represented in the current consensus sequence, remove the corresponding weight
of those edges entirely, and repeat until no further sequences remain or no
significant consensus sequence is found.</p>

<p>The consensus paper identifies a particular corner case where a consensus
sequence might terminate early; we allow this to happen.</p>

<h3 id="alignment-strings">Alignment strings</h3>

<p>Finally, to communicate the alignment results, it can still be useful
to generate a “flattened” alignment of the input and consensus sequences.</p>

<p>This is again fairly straightforwardly done once the graph is topologically
sorted.  Each node in the graph, in topological order, is assigned a column
in the final table to be generated, with rings of nodes that are aligned to
each other assigned to the same column, and nodes that are not aligned to any
others getting their own column. Then the bases are filled in, with each 
sequence (including the consensus sequences) getting their own row.</p>

<p>Because we are assigning columns to the nodes in topologically-sorted order,
the method used to generate the (non-unique) topological sort affects how
the alignments look as alignment strings, even if they are all functionally
identical.  Kahn sorting tends to interleave the results of sequences, whereas
depth-first-search necessarily visits long strings of runs in order.  DFS
then generates better looking alignment strings, so we use that approach
in the implementation below.</p>

<h3 id="simple-implementation">Simple Implementation</h3>

<p>A simple but fully functional Python implementation of the algorithms
described above <a href="https://github.com/ljdursi/poapy">can be found here</a>.
For the alignment stage, two implementations are given; one
that is quite simple to follow but is very slow; and another
that is significantly faster, but may require a little more careful
reading, as it uses numpy vectorization to improve performance.</p>

<p>Even the faster implementation is still slow – about 10 times slower
than the <a href="http://sourceforge.net/projects/poamsa/">poaV2</a> code written
in C as distributed, or closer to 20 if poaV2 is compiled with <code class="highlighter-rouge">-O3</code>
– but is nonetheless useable for small problems.</p>

<p>The simple implementation above can generate HTML with an interactive
graph visualization to explore the final partial order graph; the
visualization works particularly well on browsers with a high-performance
javascript implementation, but stops being useful for graphs with more
than a thousand nodes or so.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Partial order alignment is a powerful technique that results in a
graph containing rich information concerning the structure of the
aligned sequences, but lacks the amount of online documentation and
easy-to-explore implementations of some other methods; we hope this
helps introduce a broader audience to a more in-depth understanding
of the method.</p>

<hr />

<h3 id="references">References</h3>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://bioinformatics.oxfordjournals.org/content/18/3/452.short">Multiple sequence alignment using partial order graphs</a> (2002) by Lee, Grasso, and Sharlow <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="http://bioinformatics.oxfordjournals.org/content/19/8/999.short">Generating consensus sequences from partial order multiple sequence alignment graphs</a> (2003) by Lee <a href="#fnref:2" class="reversefootnote">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:3">
      <p><a href="http://bioinformatics.oxfordjournals.org/content/20/10/1546.short">Combining partial order alignment and progressive multiple sequence alignment increases alignment speed and scalability to very large alignment problems </a> (2004), Grasso and Lee <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="http://nar.oxfordjournals.org/content/34/20/5932.short">Multiple alignment of protein sequences with repeats and rearrangements</a> (2006) Phouong <em>et al.</em> <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p><a href="http://genome.cshlp.org/content/21/9/1512.short">Cactus: Algorithms for genome multiple sequence alignment</a> (2011) Paten <em>et al.</em> <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Later versions of excel actually allow circular dependencies in cell calculations.<sup id="fnref:6:1"><a href="#fn:6" class="footnote">6</a></sup> <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</content>
   </entry>
  
 
  
   <entry>
     <title>Aligning Nanopore Events to a Reference</title>
     <link href="http://simpsonlab.github.io/2015/04/08/eventalign/"/>
     <updated>2015-04-08T00:00:00-07:00</updated>
     <id>simpsonlab.github.io/2015/04/08/eventalign</id>
     <content type="html"><h2 id="introduction">Introduction</h2>

<p>This post describes a new module I added to our <a href="https://github.com/jts/nanopolish">nanopolish</a> software package that aligns the signal data emitted by a nanopore to a reference genome. This is in contrast to most approaches which align two DNA sequences to each other (for example a base-called read and a reference genome). To make sense of what aligning signal data to a reference genome means, I will describe at a high level my model of how nanopore sequencing works. For a more detailed and technical description, see the supplement of our <a href="http://biorxiv.org/content/early/2015/03/11/015552">preprint</a>.</p>

<h2 id="nanopore-sequencing">Nanopore Sequencing</h2>

<p>A nanopore sequencer threads a single strand of DNA through a pore embedded in a membrane. The pore allows electric current to flow from one side of the membrane to the other. As DNA transits this pore it partially blocks the flow of current, which is measured by the instrument. In Oxford Nanopore’s MinION system the measured current depends on the 5-mer that resides in the pore when the measurements are taken.</p>

<p>The MinION samples the current thousands of times per second; as 5-mers slide though the pore they should be observed in multiple samples. The MinION’s event detection software processes these samples and tries to detect points where the current level changes. These jumps indicate a new 5-mer resides in the pore. To help illustrate this I’ve reproduced a figure from our preprint below:</p>

<p><img src="/assets/simulation.svg" alt="simulation" /></p>

<p>This is a simulation from an idealized nanopore sequencing process. The black dots represent the sampled current and the red lines indicate contiguous segments that make up the detected <em>events</em>. For example the mean current was around 60 picoamps, plus a bit of noise, for the first 0.5s. The current then dropped to 40 pA for 0.1s before jumping to 52 pA and so on.</p>

<p>The event detection software writes the events to an HDF5 file. The raw kHz samples are typically not stored as the output files would be impractically large. Here’s the table of events for this simulation:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">event index</th>
      <th style="text-align: center">mean (pA)</th>
      <th style="text-align: center">length (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">60.3</td>
      <td style="text-align: center">0.521</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">40.6</td>
      <td style="text-align: center">0.112</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">52.2</td>
      <td style="text-align: center">0.356</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">54.1</td>
      <td style="text-align: center">0.051</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">61.5</td>
      <td style="text-align: center">0.291</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">72.7</td>
      <td style="text-align: center">0.015</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">49.4</td>
      <td style="text-align: center">0.141</td>
    </tr>
  </tbody>
</table>

<p>To help translate events into a DNA sequence, Oxford Nanopore provides a <em>pore model</em> which describes the expected current signal for each 5-mer. The pore model is a set of 1024 normal distributions - an example might look like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">5-mer</th>
      <th style="text-align: center"><script type="math/tex">\mu_k</script></th>
      <th style="text-align: center"><script type="math/tex">\sigma_k</script></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">AAAAA</td>
      <td style="text-align: center">53.5</td>
      <td style="text-align: center">1.3</td>
    </tr>
    <tr>
      <td style="text-align: center">AAAAC</td>
      <td style="text-align: center">54.2</td>
      <td style="text-align: center">0.9</td>
    </tr>
    <tr>
      <td style="text-align: center">…</td>
      <td style="text-align: center">…</td>
      <td style="text-align: center">…</td>
    </tr>
    <tr>
      <td style="text-align: center">TTTTG</td>
      <td style="text-align: center">65.3</td>
      <td style="text-align: center">1.8</td>
    </tr>
    <tr>
      <td style="text-align: center">TTTTT</td>
      <td style="text-align: center">67.1</td>
      <td style="text-align: center">1.4</td>
    </tr>
  </tbody>
</table>

<p>This indicates that the measured current is expected to be drawn from <script type="math/tex">\mathcal{N}(53.5, 1.3^2)</script> when AAAAA is in the pore and so on.</p>

<h2 id="inferring-bases-from-events">Inferring Bases from Events</h2>

<p>Using the pore model and the observed data we can solve a number of inference problems. For example we can infer the sequence of nucleotides that passed through the pore. This is the base calling problem. We can also infer the sequence of the genome given a set of overlapping reads. This is the consensus problem, which we addressed in our paper.</p>

<p>These inference problems are complicated by two important factors. First, the normal distributions for 5-mers overlap. There are 1024 different 5-mers but the signals typically range from about 40-70 pA. This can make it difficult to infer which 5-mer generated a particular event. This is partially mitigated by the structure of the data; a solution must respect the overlap between 5-mers so a position that is difficult to resolve may become clear when we look at subsequent events. Second, event detection is performed in real time and inevitably makes errors. Some events may not be detected if they are too short or if the signals for adjacent 5-mers of the DNA strand are very similar. The extreme case for the latter situation occurs when sequencing through long homopolymers - here we do not expect a detectable change in current. The opposite problem occurs as well. The event detector may split what should be a single event into multiple events due to noise in the system that looks like a change in current. Handling these artefacts is key to accurately inferring the DNA sequence that generated the events.</p>

<h2 id="aligning-events-to-a-reference">Aligning Events to a Reference</h2>

<p>The hidden Markov model we designed for the consensus problem had 5-mers of a proposed consensus sequence as the backbone of the HMM, with additional states and transitions to handle the skipping/splitting artefacts. In our preprint we used this HMM to calculate a consensus sequence from a set of reads. If we make a reference genome the backbone of the HMM, we can use it to align events to the reference.</p>

<p>The new <code class="highlighter-rouge">eventalign</code> module of <code class="highlighter-rouge">nanopolish</code> exposes this functionality as a command line tool.  This program takes in a set of nanopore reads aligned in base-space to a reference sequence (or draft genome assembly) and re-aligns the reads in event space.</p>

<p>The pipeline uses <code class="highlighter-rouge">bwa mem</code> alignments as a guide. We start with a normal bwa workflow:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>bwa mem -x ont2d -t 8 ecoli_k12.fasta reads.fa | samtools view -Sb - | samtools sort - alignments.sorted
samtools index alignments.sorted.bam
</code></pre>
</div>

<p>We can then realign in event space                                                   using nanopolish:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>nanopolish eventalign -r reads.fa -b alignments.sorted.bam -g ecoli_k12.fasta "gi|556503834|ref|NC_000913.3|:10000-20000" &gt; eventalign.tsv
</code></pre>
</div>

<p>The output looks like this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>contig                         position  reference_kmer  read_index  strand  event_index  event_level_mean  event_length  model_kmer  model_mean  model_stdv
gi|556503834|ref|NC_000913.3|  10000     ATTGC           1           c       27470        50.57             0.022         ATTGC       50.58       1.02
gi|556503834|ref|NC_000913.3|  10001     TTGCG           1           c       27471        52.31             0.023         TTGCG       51.68       0.73
gi|556503834|ref|NC_000913.3|  10001     TTGCG           1           c       27472        53.05             0.056         TTGCG       51.68       0.73
gi|556503834|ref|NC_000913.3|  10001     TTGCG           1           c       27473        54.56             0.011         TTGCG       51.68       0.73
gi|556503834|ref|NC_000913.3|  10002     TGCGC           1           c       27474        65.56             0.012         TGCGC       66.96       2.91
gi|556503834|ref|NC_000913.3|  10002     TGCGC           1           c       27475        69.97             0.071         TGCGC       66.96       2.91
gi|556503834|ref|NC_000913.3|  10003     GCGCT           1           c       27476        67.11             0.017         GCGCT       68.08       2.20
gi|556503834|ref|NC_000913.3|  10004     CGCTG           1           c       27477        69.47             0.052         CGCTG       69.84       1.89
</code></pre>
</div>

<p>This is the complement strand (c) of a 2D Nanopore read from Nick’s <a href="http://www.gigasciencejournal.com/content/3/1/22">E. coli data</a> aligned to E. coli K12. The first event listed (event 27470) had a measured current level of 50.57 pA. It aligns to the reference 5-mer ATTGC at position 10,000 of the reference genome. The pore model indicates that events measured for 5-mer ATTGC should come from <script type="math/tex">\mathcal{N}(50.58, 1.02^2)</script>, which matches the observed data very well. The next 3 events (27471, 27472, 27473) are all aligned to the same reference 5-mer (TTGCG) indicating that the event detector erroneously called 3 events where only one should have been emitted. Note that the current for these 3 events are all plausibly drawn from the expected distribution <script type="math/tex">\mathcal{N}(51.68, 0.73^2)</script>.</p>

<p>This output has one row for every event. If a reference 5-mer was skipped, there will be a gap in the output where no signal was observed:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>gi|556503834|ref|NC_000913.3|   10009   GCACC   1       c       27489   67.52   0.028   GCACC   66.83   2.46
gi|556503834|ref|NC_000913.3|   10011   ACCGC   1       c       27490   65.17   0.012   ACCGC   65.03   1.92
</code></pre>
</div>

<p>Here we did not observe an event for the 5-mer at position 10010.</p>

<p>This module will hopefully make it easier to work with signal-level nanopore data, and help the development of improved models. The <code class="highlighter-rouge">eventalign</code> module can be found in the latest version of <a href="https://github.com/jts/nanopolish">nanopolish</a>.</p>
</content>
   </entry>
  
 
  
   <entry>
     <title>Nanopolish v0.2.0</title>
     <link href="http://simpsonlab.github.io/2015/03/30/optimizing-hmm/"/>
     <updated>2015-03-30T00:00:00-07:00</updated>
     <id>simpsonlab.github.io/2015/03/30/optimizing-hmm</id>
     <content type="html"><p>This post describes changes I have made to <a href="https://github.com/jts/nanopolish">nanopolish</a>, our HMM-based consensus caller for Oxford Nanopore data. This post can be thought of as a long changelog with background and rationale.</p>

<h2 id="background-and-history">Background and History</h2>

<p><a href="http://nickloman.github.io/">Nick Loman</a>, Josh Quick and I started working on nanpore assembly at a hackathon at the Newton Institute in Cambridge. Our initial goal was pretty simple; we wanted to see if we could run <a href="https://github.com/thegenemyers/DALIGNER">DALIGNER</a> on nanopore data and devise a way to error correct the reads. After a lot of tinkering and “proper bioinformatics” as Nick put it (converting file formats) we were able to run <a href="http://sourceforge.net/projects/poamsa/">poa</a> on the overlapping reads that DALIGNER found. Taking poa’s consensus sequence as the error corrected read improved identity to around 92-93%. Nick was able to get <a href="http://wgs-assembler.sourceforge.net/wiki/index.php?title=Main_Page">Celera Assembler</a> running on the corrected reads and our assembly became progressively better as Nick and Josh added more data.</p>

<p>Once Nick got a single-contig assembly out of Celera Assembler we turned our attention to improving the accuracy of the final assembly. The consensus sequence that Celera Assembler called off the corrected reads had accuracy of about 98%. We knew that by working with the base-called reads, rather than the raw signal data emitted by the nanopore, we were losing a lot of information. During the winter holidays I started to write code that would use the raw current signal to call a new consensus. My initial exploratory code was in Python as the <a href="https://github.com/arq5x/poretools">poretools</a> package gave convenient access to the raw signal data encoded in ONT’s FAST5 files. I wrote a quick hidden Markov model in Python to calculate the probability of observing a sequence of nanopore signals given an arbitrary sequence. I immediately realized my Python HMM would be far too slow to run on even a bacterial genome so I decided the core algorithms would have to be written in C or C++.</p>

<p>I asked on twitter the best way to call out to a C++ library from Python and received many helpful replies (h/t to Titus Brown, Michael Crusoe and others). I settled on using <a href="https://docs.python.org/2/library/ctypes.html">ctypes</a> as this bridge between the Python frontend/poretools and the HMM in C++. I was surprised at how easy ctypes makes this - I had Python talking to a prototype C++ library in under an hour. This hybrid Python/C++ solution was just fast enough to make model development and testing possible. We spent the next month or so revising the probabilistic model of the data, developing algorithms to propose candidate consensus sequences and testing them on our E. coli data. Once the model settled we ran it on the single-contig assembly which took a few days running in parallel on Nick’s server. We wrote up a <a href="http://biorxiv.org/content/early/2015/03/11/015552">preprint</a> describing this work and posted it on BioRxiv.</p>

<h2 id="improving-the-design">Improving the design</h2>

<p>I was not satisfied with the Python/C++ hybrid design. I am sensitive to installation issues when releasing software as I have found that installing dependencies is a major source of problems for the user (although great projects like <a href="https://github.com/Homebrew/homebrew-science">homebrew-science</a> are helping a lot here). I admire Heng Li’s software where one usually just needs to run <code class="highlighter-rouge">git clone</code> and <code class="highlighter-rouge">make</code> to build the program. The initial version of nanopolish was far from this ideal as it depended on eight Python libraries that needed to be installed with <code class="highlighter-rouge">pip</code>. When moving between my local development version and Nick’s server I realized that installing these dependencies often failed. With this in mind I decided to rewrite the Python frontend in C++. To do this, I crucially needed a replacement for poretools which I used to access the raw data. Matei David in my group volunteered to help and wrote an excellent, intuitive C++ <a href="https://github.com/mateidavid/fast5">library</a> for parsing ONT’s FAST5 files.</p>

<p>There were additional benefits to this rewrite. In the Python version I again used poa to compute an initial multiple alignment and used this to seed the HMM realignment. In the C++ version I discarded this step, removing another dependency, by calculating the initial alignment directly from the BAM file using <a href="https://github.com/samtools/htslib">htslib</a>. This simplification, along with the much faster parsing of the FAST5 files provided by Matei’s library, reduced startup time from a few minutes to a few seconds. This has helped me iterate on the code much faster during development.</p>

<h2 id="improving-hmm-efficiency">Improving HMM efficiency</h2>

<p>Despite writing the HMM in C++ the first version of nanopolish was very slow. After the Python to C++ rewrite I focused on improving run time. During development I use a lightweight header-only <a href="https://github.com/jts/sga/blob/master/src/Util/Profiler.h">profiler</a> to keep track of where time is being spent in my program. As expected over 90% of the time was spent running the forward algorithm on the hidden Markov model. I used the amazing <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf</a> kernel profiler to explore this further. <code class="highlighter-rouge">perf</code> indicated that most time was in the <script type="math/tex">\log()</script> and <script type="math/tex">\exp()</script> functions. The forward algorithm on HMMs requires summing log-transformed probabilities. The naive way, <script type="math/tex">c = \log(\exp(a) + \exp(b))</script>, requires two calls to <script type="math/tex">\exp</script> and one call to <script type="math/tex">\log</script> for every state/emission pair in the HMM. This is very slow and the subject of an entire section in the classic Biological Sequence Analysis text. I remembered reading Sean Eddy’s <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002195">paper</a> on accelerating HMMER3. In this paper Sean describes how the calculation can be improved by using the transformation <script type="math/tex">c = a + \log(1 + \exp(b - a))</script> where <script type="math/tex">a \geq b</script>. On the surface this would only save a single call to <script type="math/tex">\exp</script> but Sean goes further by using a table indexed by <script type="math/tex">b - a</script> to cache <script type="math/tex">\log(1 + \exp(b - a))</script>. This method completely removes the <script type="math/tex">\exp</script> and <script type="math/tex">\log</script> calls in the inner loop of our HMM. After plugging Sean’s implementation into nanopolish we immediately had an 8-fold improvement in speed. Thanks to Sean for allowing us to use this as public domain code.</p>

<h2 id="improving-memory-layout">Improving memory layout</h2>

<p>The <code class="highlighter-rouge">perf</code> output also indicated that a lot of CPU time was wasted waiting on memory access. To improve cache usage and reduce the amount of data that is transferred over the memory bus, I reduced the precision of the floating point values from 64 bits to 32 bits. At the same time, I changed the memory layout of the nanopore event signals so that data accessed together was located in contiguous memory locations. This change simply interleaved two arrays, one storing event currents and one storing event durations, into a single array of structs. Finally, I pre-computed all of the scalings and transformations that need to be applied to the nanopore events (for example the current signal drifts over time and this needs to be corrected for) to reduce the work in the inner loop of the HMM.</p>

<h2 id="summary">Summary</h2>

<p>This collection of changes reduced the average time spent in the forward algorithm of the HMM from <script type="math/tex">3,000 {\mu}s</script> per call to <script type="math/tex">278 \mu s</script> per call for 100 input events and a 100bp sequence, an improvment of over 10x. This did not require any changes at the algorithm level, only minor code optimizations. My next goal is to make algorithmic improvements, primarily avoiding testing unlikely candidate sequences in the HMM. Version 0.2.0 of nanopolish is <a href="https://github.com/jts/nanopolish">here</a>.</p>
</content>
   </entry>
  
 

</feed>
