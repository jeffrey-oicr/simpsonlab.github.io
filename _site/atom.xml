<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Simpson Lab Blog</title>
 <link href="http://hyde.getpoole.com/atom.xml" rel="self"/>
 <link href="http://hyde.getpoole.com/"/>
 <updated>2015-03-30T15:20:16-04:00</updated>
 <id>http://hyde.getpoole.com</id>
 <author>
   <name>Jared Simpson</name>
   <email></email>
 </author>

 
 <entry>
   <title>Revising Nanopolish</title>
   <link href="http://hyde.getpoole.com/2015/03/30/optimizing-hmm/"/>
   <updated>2015-03-30T00:00:00-04:00</updated>
   <id>http://hyde.getpoole.com/2015/03/30/optimizing-hmm</id>
   <content type="html">&lt;p&gt;Last month Nick Loman, Josh Quick and I posted a &lt;a href=&quot;http://biorxiv.org/content/early/2015/03/11/015552&quot;&gt;preprint&lt;/a&gt; on assembling E. coli using Oxford Nanopore data. One of the challenges of working with a new sequencing platform is understanding and handling the error model. We spent a lot of time working on algorithms that use the raw signal data emitted by the nanopore to compute the final consensus sequence for the assembly. Our approach is implemented in a software package called &lt;a href=&quot;https://github.com/jts/nanopolish&quot;&gt;nanopolish&lt;/a&gt;. At the core of this software is a hidden Markov model that calculates the probability of observing a sequence of nanopore &lt;em&gt;events&lt;/em&gt; given a proposed sequence. I’m not going to discuss the details of the probabilistic model in this post - this can be found in the supplement of our preprint - rather I’m going to discuss changes and optimizations to our implementation.&lt;/p&gt;

&lt;p&gt;I started to work on signal-level algorithms for Oxford Nanopore data during the 2014 winter holidays. My initial exploratory code was in Python as the &lt;a href=&quot;https://github.com/arq5x/poretools&quot;&gt;poretools&lt;/a&gt; package gave convenient access to the raw signal data encoded in ONT’s FAST5 files. My Python explorations convinced me that there was enough information in the raw signal to substantially improve the quality of our draft assembly so I set off to write a consensus algorithm. After very briefly writing a Python implementation of a PairHMM, I rewrote the core algorithms in C++ using the &lt;a href=&quot;https://docs.python.org/2/library/ctypes.html&quot;&gt;ctypes&lt;/a&gt; library as a bridge between the Python frontend/poretools and the HMM in C++ (h/t to Titus Brown, Michael Crusoe and others on twitter for suggesting this). This version was fast enough that we could easily test new models and ideas but very little effort was spent on performance. Once we settled on a model it took a few days to compute the final consensus using Nick’s server.&lt;/p&gt;

&lt;p&gt;After submitting the paper I spent time improving the code. My first goal was to make nanopolish easier to install, develop and run. Using Python worked well during prototyping - I could easily try out new ideas without writing much code - but as the program matured it became cumbersome to maintain the C++/Python bridge. Startup time was a problem as well; parsing the FAST5 files for 20 reads could take a few minutes to complete. This led to long write/compile/test cycles which frustrated me. Finally, the Python code depended on eight libraries. &lt;code&gt;pip install&lt;/code&gt; very rarely worked for me without having to debug some package installation error. To fix these annoying issues I decided to rewrite all of the Python code in C++. Matei David provided a very nice C++ &lt;a href=&quot;https://github.com/mateidavid/fast5&quot;&gt;library&lt;/a&gt; to read the ONT FAST5 files. To read BAM files I used &lt;a href=&quot;https://github.com/samtools/htslib&quot;&gt;htslib&lt;/a&gt; for the first time.&lt;/p&gt;

&lt;p&gt;My second goal was to improve the efficiency of the hidden Markov model calculations.  I used the &lt;a href=&quot;https://perf.wiki.kernel.org/index.php/Tutorial&quot;&gt;perf&lt;/a&gt; kernel profiler to check for hotspots in the code. As I expected, most of the time was spent in the &lt;script type=&quot;math/tex&quot;&gt;\log()&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\exp()&lt;/script&gt; functions. When using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Forward_algorithm&quot;&gt;Forward Algorithm&lt;/a&gt; on a hidden Markov model, one constantly needs to sum log-transformed probabilities. The naive way, &lt;script type=&quot;math/tex&quot;&gt;c = \log(\exp(a) + \exp(b))&lt;/script&gt;, requires two calls to &lt;script type=&quot;math/tex&quot;&gt;\exp&lt;/script&gt; and one call to &lt;script type=&quot;math/tex&quot;&gt;\log&lt;/script&gt; for every state/emission pair in the HMM. This is very slow and the subject of an entire section in the classic Biological Sequence Analysis text. I remembered reading Sean Eddy’s &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002195&quot;&gt;paper&lt;/a&gt; on accelerating HMMER3. In this paper Sean describes how the calculation can be improved by using the transformation &lt;script type=&quot;math/tex&quot;&gt;c = a + \log(1 + \exp(b - a))&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;a \geq b&lt;/script&gt;. On the surface this would only save a single call to &lt;script type=&quot;math/tex&quot;&gt;\exp()&lt;/script&gt; but Sean goes further by using a table indexed by &lt;script type=&quot;math/tex&quot;&gt;b - a&lt;/script&gt; to cache &lt;script type=&quot;math/tex&quot;&gt;\log(1 + \exp(b - a))&lt;/script&gt;. This method completely removes the &lt;script type=&quot;math/tex&quot;&gt;\exp()&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\log()&lt;/script&gt; calls in the inner loop of our HMM. After plugging Sean’s implementation into nanopolish we immediately had an 8-fold improvement in speed. Thanks to Sean for allowing us to use this as public domain code.&lt;/p&gt;

&lt;p&gt;Next I reduced the precision of the floating point values from 64-bits to 32-bits to reduce cache usage and how much data needed to be transferred over the bus. I also reorganized the data layout in memory to reduce the number of random accesses. Finally, I pre-computed all of the scalings and transformations that need to be applied to the nanopore events (for example the current signal drifts over time and this needs to be corrected for) to reduce the work in the inner loop of the HMM. Altogether, these changes reduce run time by a factor of 12 over the initial version. There is a room for further improvement at the algorithm level, mainly how candidate consensus sequences are proposed, but those changes will have to wait. The latest version is now on github here (todo).&lt;/p&gt;
</content>
 </entry>
 

</feed>
